{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 09:48:17.470071: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 09:48:17.541287: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import ee \n",
    "import ee_utils\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive Temperatures (Earth Engine LST) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ADAPT THESE PARAMETERS ==========\n",
    "# To export to Google Drive, uncomment the next 2 lines\n",
    "EXPORT = ''\n",
    "BUCKET = None\n",
    "# export location parameters\n",
    "ERA5_EXPORT_FOLDER = ''\n",
    "CSV_PATH = '../data/dataset_2013+.csv'\n",
    "BANDS = ['mean_2m_air_temperature', 'minimum_2m_air_temperature', 'maximum_2m_air_temperature']\n",
    "# image export parameters\n",
    "PROJECTION = 'EPSG:3857'  # see https://epsg.io/3857\n",
    "SCALE = 30                # export resolution: 30m/px\n",
    "EXPORT_TILE_RADIUS = 3  # We only need the central values here\n",
    "CHUNK_SIZE = None    # set to a small number (<= 50) if Google Earth Engine reports memory errors; \n",
    "csv = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_images(\n",
    "        df: pd.DataFrame,\n",
    "        collection: ee.ImageCollection,\n",
    "        country: str,\n",
    "        year: int,\n",
    "        export_folder: str,\n",
    "        chunk_size = 1,\n",
    " ):\n",
    "    '''\n",
    "    Args\n",
    "    - df: pd.DataFrame, contains columns ['lat', 'lon', 'country', 'year']\n",
    "    - country: str, together with `year` determines the survey to export\n",
    "    - year: int, together with `country` determines the survey to export\n",
    "    - export_folder: str, name of folder for export\n",
    "    - chunk_size: int, optionally set a limit to the # of images exported per TFRecord file\n",
    "        - set to a small number (<= 50) if Google Earth Engine reports memory errors\n",
    "\n",
    "    Returns: dict, maps task name tuple (export_folder, country, year, chunk) to ee.batch.Task\n",
    "    '''\n",
    "\n",
    "    subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "    if chunk_size is None:\n",
    "        chunk_size = len(subset_df)\n",
    "    num_chunks = int(math.ceil(len(subset_df) / chunk_size))\n",
    "    tasks = {}\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_slice = slice(i * chunk_size, (i+1) * chunk_size - 1)  # df.loc[] is inclusive\n",
    "        fc = ee_utils.df_to_fc(subset_df.loc[chunk_slice, :])\n",
    "        for prev_year in range(year-4, year+1):\n",
    "            for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "                start_date, end_date = str(prev_year)+'-'+month+'-01',str(prev_year)+'-'+month+'-28'\n",
    "                roi = fc.geometry()\n",
    "                collection_ave = collection.select(BANDS[0]).filterDate(start_date, end_date).filterBounds(roi)\n",
    "                ave = collection_ave.median()\n",
    "                ave = ee_utils.add_latlon(ave)\n",
    "\n",
    "                fname = f'{country}_{year}_{prev_year}_{i:02d}'\n",
    "                tasks[(export_folder, country, prev_year, i)] = ee_utils.get_array_patches(\n",
    "                        img=ave, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
    "                        points=fc, export='drive',\n",
    "                        prefix=export_folder, fname=fname+'_'+month,\n",
    "                        bucket=None)\n",
    "        return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = ee.ImageCollection(\"ECMWF/ERA5/MONTHLY\")\n",
    "dataset = pd.read_csv('../data/dataset_2013+.csv')\n",
    "dataset_ = list(dataset.groupby(['country', 'year']).groups.keys())\n",
    "tasks = {}\n",
    "for country, year in tqdm(dataset_):\n",
    "    print(country, year)\n",
    "    new_tasks = export_images(\n",
    "        df=dataset, collection=collection, country=country, year=year,\n",
    "        export_folder=ERA5_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = ['minimum_2m_air_temperature', 'maximum_2m_air_temperature','mean_2m_air_temperature']\n",
    "\n",
    "BANDS_ORDER = ['minimum_2m_air_temperature', 'maximum_2m_air_temperature','mean_2m_air_temperature']\n",
    "\n",
    "\n",
    "EXPORT_FOLDER = '../data/additional_data/temperature'\n",
    "PROCESSED_FOLDER = '../data/additional_data/temperature'\n",
    "def validate_and_split_tfrecords(\n",
    "        tfrecord_paths,\n",
    "        out_dir: str,\n",
    "        df: pd.DataFrame,\n",
    "        country,\n",
    "        year\n",
    "        ) -> None:\n",
    "    '''Validates and splits a list of exported TFRecord files (for a\n",
    "    given country-year survey) into individual TFrecords, one per cluster.\n",
    "\n",
    "    \"Validating\" a TFRecord comprises of 2 parts\n",
    "    1) verifying that it contains the required bands\n",
    "    2) verifying that its other features match the values from the dataset CSV\n",
    "\n",
    "    Args\n",
    "    - tfrecord_paths: list of str, paths to exported TFRecords files\n",
    "    - out_dir: str, path to dir to save processed individual TFRecords\n",
    "    - df: pd.DataFrame, index is sequential and starts at 0\n",
    "    '''\n",
    "    # Create an iterator over the TFRecords file. The iterator yields\n",
    "    # the binary representations of Example messages as strings.\n",
    "    options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "\n",
    "    # cast float64 => float32 and str => bytes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == object:  # pandas uses 'object' type for str\n",
    "            df[col] = df[col].astype(bytes)\n",
    "\n",
    "   \n",
    "    progbar = tqdm(total=len(df))\n",
    "\n",
    "    for tfrecord_path in tfrecord_paths:\n",
    "        iterator = tf.compat.v1.io.tf_record_iterator(tfrecord_path, options=options)\n",
    "        for record_str in iterator:\n",
    "            # parse into an actual Example message\n",
    "            ex = tf.train.Example.FromString(record_str)\n",
    "            feature_map = ex.features.feature\n",
    "            index = str(int(feature_map[\"cluster\"].float_list.value[0]))\n",
    "            for band in REQUIRED_BANDS:\n",
    "                assert band in feature_map, f'Band \"{band}\" not in record {index} of {tfrecord_path}'\n",
    "#             serialize to string and write to file\n",
    "            month = tfrecord_path[-14:-12]\n",
    "            out_path = os.path.join(out_dir, f'{index}'+\"_\"+month+'.tfrecord.gz')  # all surveys have < 1e6 clusters\n",
    "            with tf.io.TFRecordWriter(out_path, options=options) as writer:\n",
    "                writer.write(ex.SerializeToString())\n",
    "            progbar.update(1)\n",
    "    progbar.close()\n",
    "    \n",
    "\n",
    "def process_dataset(csv_path: str, input_dir: str, processed_dir: str) -> None:\n",
    "    '''\n",
    "    Args\n",
    "    - csv_path: str, path to CSV of DHS or LSMS clusters\n",
    "    - input_dir: str, path to TFRecords exported from Google Earth Engine\n",
    "    - processed_dir: str, folder where to save processed TFRecords\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, float_precision='high', index_col=False)\n",
    "    surveys = list(df.groupby(['country', 'year']).groups.keys())  # (country, year) tuples\n",
    "   \n",
    "    # print(year, type(year))\n",
    "    for country, year in surveys:        \n",
    "        # Checking inside potential subfolders\n",
    "        for prev_year in range(year-4, year+1):\n",
    "            country_year = f'{country}_{year}_{prev_year}'\n",
    "            print('Processing:', country_year)\n",
    "            tfrecord_paths = glob(os.path.join(input_dir, country_year+'*.tfrecord.gz'))\n",
    "            tfrecord_paths += glob(os.path.join(input_dir, \"*\", country_year + '*.tfrecord.gz'))\n",
    "            tfrecord_paths += glob(os.path.join(input_dir, \"*\",\"*\", country_year + '*.tfrecord.gz'))\n",
    "\n",
    "            out_dir = os.path.join(processed_dir, country_year)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "            validate_and_split_tfrecords(\n",
    "            tfrecord_paths=tfrecord_paths, out_dir=out_dir, df=subset_df, country=country, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    csv_path='../data/dataset_viirs_only.csv',\n",
    "    input_dir=EXPORT_FOLDER,\n",
    "    processed_dir=PROCESSED_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV              = os.path.join( \"..\", \"data\", \"dataset_viirs_only.csv\" )\n",
    "RECORDS_DIR      = os.path.join( \"..\", \"data\", \"additional_data\", \"temperature\", \"\")\n",
    "TIF_DIR          = os.path.join( \"..\", \"data\", \"additional_data\", \"temperature\", \"\" )\n",
    "\n",
    "csv = pd.read_csv(CSV)\n",
    "records = dict()\n",
    "for year in csv.year.unique():\n",
    "    sub_year = csv[ csv.year == year ]\n",
    "    for prev_year in range(year-4, year+1):\n",
    "        records[year, prev_year] = dict()\n",
    "        for country in sub_year.country.unique():\n",
    "            sub_country = sub_year[ sub_year.country == country ].copy()\n",
    "            pattern ='../'+RECORDS_DIR+str(country)+\"_\"+str(year)+\"_\"+str(prev_year)+\"/\"\n",
    "            records[year,prev_year][country] = glob(pattern)\n",
    "            print(pattern)\n",
    "\n",
    "    break\n",
    "def decompress_tfrecord(tfrecord_archive):\n",
    "    with gzip.open(tfrecord_archive, 'rb') as f_in:\n",
    "        # WITHOUT .GZ\n",
    "        with open(tfrecord_archive[:-3], 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    return tfrecord_archive[:-3]\n",
    "\n",
    "def tensor_to_string(data, variable):\n",
    "    filename = (data[variable].numpy())[0][0]\n",
    "    return str(filename).replace(\".\",\"\")\n",
    "\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTOR       = {\n",
    "                'cluster':\"float\",\n",
    "                'lat':\"float\", \n",
    "                \"lon\":\"float\",\n",
    "                'wealthpooled':\"float\",\n",
    "                'minimum_2m_air_temperature':'float',\n",
    "                'maximum_2m_air_temperature':'float',\n",
    "                'mean_2m_air_temperature':'float'\n",
    "    \n",
    "              } \n",
    "\n",
    "BANDNAMES = ['mean_2m_air_temperature', 'minimum_2m_air_temperature','maximum_2m_air_temperature']\n",
    "\n",
    "def tfrecord_to_tif(data, filename, mins, maxs):\n",
    "    arrays = [] \n",
    "    for i in range(3):\n",
    "        new_arr = data[BANDNAMES[i]].numpy().reshape((3,3))\n",
    "        arrays.append(new_arr)\n",
    "        mins[i] = min(mins[i], new_arr.min())\n",
    "        maxs[i] = max(maxs[i], new_arr.max())\n",
    "    \n",
    "    arr = np.swapaxes(np.array(arrays), 0, 2 )\n",
    "    tif_path = TIF_DIR + filename\n",
    "    tif = rasterio.open(tif_path, 'w', driver='GTiff',\n",
    "                            height = arr.shape[0], width = arr.shape[1],\n",
    "                            count=8, dtype=str(arr.dtype),\n",
    "                            crs='epsg:3857',\n",
    "                            transform=None)\n",
    "    tif.write(arr[:,:,0],1)\n",
    "    tif.close()\n",
    "    return mins, maxs\n",
    "\n",
    "def read_record(data):\n",
    "    result = []\n",
    "    for i in range(3):\n",
    "        # READ CENTRAL VALUE\n",
    "        result.append(data[BANDNAMES[i]].numpy().reshape((3,3))[1,1])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = dict()\n",
    "month_to_index = {\n",
    "        '01':0,\n",
    "        '02':1,\n",
    "        '03':2,\n",
    "        '04':3,\n",
    "        '05':4,\n",
    "        '06':5,\n",
    "        '07':6,\n",
    "        '08':7,\n",
    "        '09':8,\n",
    "        '10':9,\n",
    "        '11':10,\n",
    "        '12':11,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temperature = 0.\n",
    "std_temperature = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, prev_year in records:\n",
    "    print(year, prev_year)\n",
    "    for country in records[year, prev_year]:\n",
    "        print(country)\n",
    "        for tfrecord_archive in tqdm(records[year,prev_year][country]):\n",
    "            if tfrecord_archive[-3:] == '.gz':\n",
    "                tfrecord = decompress_tfrecord(tfrecord_archive=tfrecord_archive)\n",
    "                tfrecord = tfrecord_archive[:-3]\n",
    "            else:\n",
    "                tfrecord = tfrecord_archive\n",
    "            dataset = TFRecordDataset(tfrecord, index_path=None, description=DESCRIPTOR)\n",
    "            loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "            iterator = iter(loader)\n",
    "            tfrecord = tfrecord.split('/')[-1]\n",
    "            month=tfrecord[-11:-9] \n",
    "            cluster=tfrecord[:-12]\n",
    "            while (data := next(iterator, None)) is not None:\n",
    "                val = read_record(data)\n",
    "                print('VAL  :::: ', val)\n",
    "                if (country, year, prev_year, int(cluster)) not in temperatures:\n",
    "                    temperatures[ (country, year, prev_year, int(cluster)) ] = [[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "                temperatures[(country, year, prev_year, int(cluster))][month_to_index[month]]=val\n",
    "        with open('temperatures.pickle', 'wb') as handle:\n",
    "            pickle.dump(temperatures, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check temperature dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/additional_data/temperatures.pickle', 'rb') as handle:\n",
    "    temperatures = pickle.load( handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del temperatures['mins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([])\n",
    "for k in temperatures.keys():\n",
    "    if arr==np.array([]):\n",
    "        arr = np.array(temperatures[k]).flatten()\n",
    "    else:\n",
    "        arr = np.concatenate((arr, np.array(temperatures[k]).flatten()))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.mean(arr)\n",
    "std=np.std(arr)\n",
    "mean,std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive Precipitations (FOA WAPOR PCP - from CHIRPS catalog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRECIPITATIONS - not expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/dataset_2013+.csv')\n",
    "PATH = os.path.join('../data', 'additional_data','precipitation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()\n",
    "dataset.to_csv('../data/dataset_additional.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"precipitation\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_island_coordinates(x,y):\n",
    "    '''return the closest valid points when dealing with islands due to coarse tif resolution'''\n",
    "    # Tanzania\n",
    "    if int(y)==-5 or int(y)==-6 and int(x)==39:\n",
    "        return 39.29, -5.98\n",
    "    # Sierra Leone \n",
    "    if (int(x)==-13 or int(x)==-12) and (int(y)==8 or int(y)==7):\n",
    "        return -12.7, 7.8\n",
    "    # Senegal \n",
    "    if int(x) in [-12,-13,-16,-17] and int(y) in [12,13,14,15,16]:\n",
    "        return x-1, y\n",
    "    # Mozambique \n",
    "    if (int(x), int(y)) in [(32,-25),(40,-12)]:\n",
    "        return int(x), int(y)\n",
    "    if (int(x), int(y))== (34, -19):\n",
    "        return 34.80, -19.80\n",
    "    # Madagascar\n",
    "    if (int(x)==43 and int(y)==-23):\n",
    "        return x+1, y\n",
    "    if (int(x) in (48,49) and int(y) in (-12,-13)):\n",
    "        return x, y-2 \n",
    "    # Guinea \n",
    "    if int(x)==-13 and int(y)==9:\n",
    "        return x+0.5, y+0.5\n",
    "    # Cote d'Ivoir\n",
    "    if int(x)==-6 and int(y)==4:\n",
    "        return x+0.5, y+0.5\n",
    "    # Benin\n",
    "    if int(x) in (1,2) and int(y)==6:\n",
    "        return x, y+0.5\n",
    "    # Angola\n",
    "    if int(x)==-13 and int(y) in (-8,-12):\n",
    "        return x+0.5, y\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates precipitation pikle dictionary with manually corrected island coordinates\n",
    "\n",
    "precipitations = dict()\n",
    "dataset = pd.read_csv('../data/dataset_viirs_only.csv')\n",
    "\n",
    "for year in tqdm(dataset.year.unique()):\n",
    "    df = dataset[dataset.year==year]\n",
    "    print(year)\n",
    "    for prev_year in range(year-4,year+1):\n",
    "        # sorted months\n",
    "        monthly_tifs = glob.glob(os.path.join(PATH, str(prev_year)+\"*.tif\"))\n",
    "        for tif in monthly_tifs:\n",
    "            with rio.open(tif) as src: \n",
    "                for country in df.country.unique():\n",
    "                    df_country = df[df.country==country]\n",
    "                    for cluster in df_country.cluster.unique():\n",
    "                        if (country,year,prev_year, cluster) not in precipitations:\n",
    "                            precipitations[country,year,prev_year, cluster] = []\n",
    "                        if precipitations[(country,year,prev_year, cluster)] == None:\n",
    "                            precipitations[country,year,prev_year, cluster] = []\n",
    "                        vector = []\n",
    "                        row = df_country.loc[(df_country['cluster'] == cluster)]\n",
    "                        x = float(row.at[row.index[0],'lon'])\n",
    "                        y = float(row.at[row.index[0],'lat'])\n",
    "                        x, y = correct_island_coordinates(x,y)      \n",
    "                        for val in src.sample([(x, y)]): \n",
    "                            # THE ORIGINAL RASTERS HAVE TOO COARSE RESOLUTION TO CAPTURE SMALL ISLANDS AND COASTLINES AS VALID COORDINATES\n",
    "                            # WE TAKE THE THE CLOSEST COASTAL POINT IN THIS CASE \n",
    "                            vector=val[0]\n",
    "                        precipitations[(country,year,prev_year, cluster)].append(vector)\n",
    "                with open('../data/additional_data/precipitations.pickle', 'wb') as handle:\n",
    "                    pickle.dump(precipitations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRECIPITATIONS - expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates precipitation pikle dictionary with expanded precipitation rasters (using same method as non expanded precipitations)\n",
    "\n",
    "precipitations = dict()\n",
    "dataset = pd.read_csv('../data/dataset_2013+.csv')\n",
    "PATH = os.path.join('../data', 'additional_data','precipitation')\n",
    "\n",
    "for year in tqdm(dataset.year.unique()):\n",
    "    df = dataset[dataset.year==year]\n",
    "    print(year)\n",
    "    for prev_year in range(year-4,year+1):\n",
    "        # sorted months\n",
    "        monthly_tifs = glob.glob(os.path.join(PATH, str(prev_year)+\"*.tif\"))\n",
    "        for tif in monthly_tifs:\n",
    "            with rio.open(tif) as src: \n",
    "                for country in df.country.unique():\n",
    "                    df_country = df[df.country==country]\n",
    "                    for cluster in df_country.cluster.unique():\n",
    "                        if (country,year,prev_year, cluster) not in precipitations:\n",
    "                            precipitations[country,year,prev_year, cluster] = []\n",
    "                        if precipitations[(country,year,prev_year, cluster)] == None:\n",
    "                            precipitations[country,year,prev_year, cluster] = []\n",
    "                        vector = []\n",
    "                        row = df_country.loc[(df_country['cluster'] == cluster)]\n",
    "                        x = float(row.at[row.index[0],'lon'])\n",
    "                        y = float(row.at[row.index[0],'lat'])\n",
    "                        #x, y = correct_island_coordinates(x,y)      \n",
    "                        for val in src.sample([(x, y)]): \n",
    "\n",
    "                            vector=val[0]\n",
    "                        precipitations[(country,year,prev_year, cluster)].append(vector)\n",
    "                with open('../data/additional_data/precipitations.pickle', 'wb') as handle:\n",
    "                    pickle.dump(precipitations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out created dict.\n",
    "\n",
    "with open('../data/additional_data/precipitation_1.pickle', 'rb') as f:\n",
    "    dic = pickle.load(f)\n",
    "\n",
    "dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new variable from raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/dataset_2013+.csv')\n",
    "DATA_DIR='../data/additional_data/conflicts/'\n",
    "DICT_NAME='../data/additional_data/conflict.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We subdivide the dataset rather than iterating through rows naively\n",
    "# to limit the number of raster-reading operation, which is the bottleneck of this preprocessing step\n",
    "series_dict = dict()\n",
    "for year in tqdm(dataset.year.unique()):\n",
    "    df = dataset[dataset.year==year]\n",
    "    print(year)\n",
    "    for prev_year in range(year-4,year+1):\n",
    "        # sorted months\n",
    "        # we leave the tif files opened and sample all observations at once\n",
    "        tif = glob.glob(os.path.join(DATA_DIR, str(prev_year)+\"*.tif\"))[0]    \n",
    "        with rio.open(tif) as src: \n",
    "            for country in df.country.unique():\n",
    "                df_country = df[df.country==country]\n",
    "                for cluster in df_country.cluster.unique():\n",
    "                    if (country,year,prev_year, cluster) not in series_dict:\n",
    "                        series_dict[country,year,prev_year, cluster] = []\n",
    "                    if series_dict[(country,year,prev_year, cluster)] == None:\n",
    "                        series_dict[country,year,prev_year, cluster] = []\n",
    "                    vector = []\n",
    "                    row = df_country.loc[(df_country['cluster'] == cluster)]\n",
    "                    x = float(row.at[row.index[0],'lon'])\n",
    "                    y = float(row.at[row.index[0],'lat'])\n",
    "                    # SEVERAL BANDS ?\n",
    "                    for val in src.sample([(x, y)]): \n",
    "                        vector=val[0]\n",
    "                    series_dict[(country,year,prev_year, cluster)].append(vector)\n",
    "            with open(DICT_NAME, 'wb') as handle:\n",
    "                pickle.dump(series_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace neg values with 0 in pickle dict\n",
    "\n",
    "DICT_NAME='../data/additional_data/conflict.pickle'\n",
    "\n",
    "# Open pickle file\n",
    "with open(DICT_NAME, 'rb') as pickle_file:\n",
    "    loaded_dict = pickle.load(pickle_file)\n",
    "\n",
    "# Function to replace neg values with 0\n",
    "def replace_neg(value):\n",
    "\n",
    "    # If value is a list\n",
    "    if isinstance(value, list):\n",
    "        return [max(0,x) for x in value]\n",
    "    else:\n",
    "        return max(0, value)\n",
    "    \n",
    "# Function to recursively apply the replacement fct to dict\n",
    "def replace_recursive(d):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            d[key] = replace_recursive(value)\n",
    "        else: \n",
    "            d[key] = replace_neg(value)\n",
    "    return d\n",
    "\n",
    "# Replace negatove values with 0 in the dictionnary \n",
    "conflict_noneg = replace_recursive(loaded_dict)\n",
    "\n",
    "# Save the new dict \n",
    "output_PATH = \"../data/additional_data/conflict_noneg.pickle\"\n",
    "\n",
    "with open(output_PATH, 'wb') as file:\n",
    "    pickle.dump(loaded_dict, file)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Neg values replaced with 0 in new pickle dict\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save `mean` & `std` to normalizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  0.0 max:  17320.0\n"
     ]
    }
   ],
   "source": [
    "# For precipitations\n",
    "\n",
    "DICT_NAME='../data/additional_data/precipitation_1.pickle'\n",
    "VARIABLE='precipitation'\n",
    "\n",
    "from helper import get_mean_std_from_dict\n",
    "\n",
    "with open(DICT_NAME, 'rb') as f:\n",
    "    series_dict = pickle.load(f)\n",
    "mean, std = get_mean_std_from_dict(series_dict)\n",
    "mean, std\n",
    "\n",
    "with open('../datasets/normalizer.pkl', 'rb') as f:\n",
    "    normalizer = pickle.load(f)\n",
    "normalizer[VARIABLE] = mean, std\n",
    "with open('../datasets/normalizer.pkl', 'wb') as f:\n",
    "    pickle.dump(normalizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conflict\n",
    "\n",
    "DICT_NAME='../data/additional_data/conflict_noneg.pickle'\n",
    "VARIABLE='conflict'\n",
    "\n",
    "from helper import get_mean_std_from_dict\n",
    "\n",
    "with open(DICT_NAME, 'rb') as f:\n",
    "    series_dict = pickle.load(f)\n",
    "mean, std = get_mean_std_from_dict(series_dict)\n",
    "mean, std\n",
    "\n",
    "\n",
    "with open('../datasets/normalizer.pkl', 'rb') as f:\n",
    "    normalizer = pickle.load(f)\n",
    "normalizer[VARIABLE] = mean, std\n",
    "with open('../datasets/normalizer.pkl', 'wb') as f:\n",
    "    pickle.dump(normalizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'landsat_+_nightlights': (array([5.17201265e-02, 8.82524713e-02, 1.02674783e-01, 2.64303597e-01,\n",
       "         2.49776030e-01, 1.71501024e-01, 3.00020234e+02, 1.59417214e+00]),\n",
       "  array([6.19937578e-04, 1.27377769e-03, 3.33014929e-03, 4.55055797e-03,\n",
       "         9.47793062e-03, 8.13588032e-03, 2.26191530e+01, 1.01391193e+00])),\n",
       " 'temperature': (299.78521219889325, 2.5680043231102125),\n",
       " 'conflict': (2.8288902449655655, 31.847525639418162),\n",
       " 'precipitation': (1029.2007072616948, 1219.951013718319)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../datasets/normalizer.pkl', 'rb') as f:\n",
    "    normalizer = pickle.load(f)\n",
    "normalizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove 'random' from Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/normalizer_random.pkl', 'rb') as f:\n",
    "    normalizer = pickle.load(f)\n",
    "key_to_remove =['random']\n",
    "for key in key_to_remove:\n",
    "    if key in normalizer:\n",
    "        del normalizer[key]\n",
    "\n",
    "# Save the modified dictionary\n",
    "with open('../datasets/normalizer.pkl', 'wb') as f:\n",
    "    pickle.dump(normalizer, f)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why do we have negative values in conflict dict ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of NA in conflict RASTERS\n",
    "\n",
    "import rasterio\n",
    "\n",
    "Raster_PATH  = \"../data/additional_data/conflicts/2019.tif\"\n",
    "\n",
    "with rasterio.open(Raster_PATH) as src:\n",
    "    raster_data = src.read(1)\n",
    "\n",
    "    na_count = (raster_data == src.nodata).sum()\n",
    "\n",
    "    print(na_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of pixels <0 in conflict RASTERS\n",
    "\n",
    "Raster_PATH  = \"../data/additional_data/conflicts/2018.tif\"\n",
    "\n",
    "with rasterio.open(Raster_PATH) as src:\n",
    "    raster_data = src.read(1)\n",
    "\n",
    "    neg_count = (raster_data <0).sum()\n",
    "\n",
    "    print(neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print conflict dict\n",
    "\n",
    "DICT_NAME='../data/additional_data/conflict.pickle'\n",
    "with open(DICT_NAME, 'rb') as pickle_file:\n",
    "    loaded_dict = pickle.load(pickle_file)\n",
    "\n",
    "for key, value in loaded_dict.items():\n",
    "    print(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of pixels <0 in conflict PICKLE\n",
    "\n",
    "DICT_NAME='../data/additional_data/conflict.pickle'\n",
    "with open(DICT_NAME, 'rb') as pickle_file:\n",
    "    loaded_dict = pickle.load(pickle_file)\n",
    "    \n",
    "\n",
    "for key, value in loaded_dict.items():\n",
    "    if isinstance(value, list):\n",
    "        for element in value:\n",
    "            if element < 0:\n",
    "                print(f\"Key: {key}, Element: {element}\")\n",
    "    elif value < 0:\n",
    "        print(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meand and STD of pixels <0 in conflict PICKLE\n",
    "\n",
    "DICT_NAME='../data/additional_data/conflict.pkl'\n",
    "with open(DICT_NAME, 'rb') as pickle_file:\n",
    "    loaded_dict = pickle.load(pickle_file)\n",
    "    \n",
    "negative_elements = []\n",
    "\n",
    "# Iterate through the dict and collect neg elements\n",
    "for key, value in loaded_dict.items():\n",
    "\n",
    "    # Value is a list, so check each element in the list\n",
    "    if isinstance(value, list):\n",
    "        for element in value:\n",
    "            if element < 0:\n",
    "                # Round neg \n",
    "                element = round(element, 3)\n",
    "                negative_elements.append(element)\n",
    "\n",
    "    # In case value is not a list\n",
    "    elif value < 0:\n",
    "        # Round neg \n",
    "        value = round(value, 3)\n",
    "        negative_elements.append(value)\n",
    "\n",
    "# Calculate mean and std of neg values and elements\n",
    "if negative_elements:\n",
    "    mean_neg = np.mean(negative_elements)\n",
    "    std_neg = np.std(negative_elements)\n",
    "    print(f\"Mean of neg elements: {mean_neg:.3f}\")\n",
    "    print(f\"STD of neg elements: {std_neg:.3f}\")\n",
    "else:\n",
    "    print(\"No neg elem\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine best epoch for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print the structure of a dic\n",
    "\n",
    "def print_dict_str(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(\" \" * indent + f\"{key}: (dict)\")\n",
    "            print_dict_str(value, indent + 4)\n",
    "        else:\n",
    "            print(\" \" * indent + f\"{key}: {type(value).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: (dict)\n",
      "    train_loss: list\n",
      "    train_r2: list\n",
      "    test_loss: list\n",
      "    test_r2: list\n",
      "B: (dict)\n",
      "    train_loss: list\n",
      "    train_r2: list\n",
      "    test_loss: list\n",
      "    test_r2: list\n",
      "C: (dict)\n",
      "    train_loss: list\n",
      "    train_r2: list\n",
      "    test_loss: list\n",
      "    test_r2: list\n",
      "D: (dict)\n",
      "    train_loss: list\n",
      "    train_r2: list\n",
      "    test_loss: list\n",
      "    test_r2: list\n",
      "E: (dict)\n",
      "    train_loss: list\n",
      "    train_r2: list\n",
      "    test_loss: list\n",
      "    test_r2: list\n"
     ]
    }
   ],
   "source": [
    "# print structure of a dic\n",
    "\n",
    "with open('../models/results/conf_t/msnlt_conf_t2.pkl', 'rb') as f:\n",
    "    dic = pickle.load(f)\n",
    "\n",
    "print_dict_str(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epochs based on test_r2:\n",
      "Fold A: Epoch 60 (Test R-squared)\n",
      "Fold B: Epoch 60 (Test R-squared)\n",
      "Fold C: Epoch 55 (Test R-squared)\n",
      "Fold D: Epoch 57 (Test R-squared)\n",
      "Fold E: Epoch 59 (Test R-squared)\n"
     ]
    }
   ],
   "source": [
    "# Best epoch\n",
    "\n",
    "with open('../models/results/conf_t/ts_conf_t3.pkl', 'rb') as f:\n",
    "    dic = pickle.load(f)\n",
    "\n",
    "\n",
    "best_epochs = {}\n",
    "\n",
    "# Iterate through the folds\n",
    "for fold, fold_results in dic.items():\n",
    "    # Find the epoch with the minimum test_loss\n",
    "    best_test_loss_epoch = np.argmin(fold_results['test_loss']) + 1  # Adjust for 0-based index\n",
    "    \n",
    "    # Find the epoch with the maximum test_r2\n",
    "    best_test_r2_epoch = np.argmax(fold_results['test_r2']) + 1  # Adjust for 0-based index\n",
    "    \n",
    "    best_epochs[fold] = {\n",
    "        'best_test_loss_epoch': best_test_loss_epoch,\n",
    "        'best_test_r2_epoch': best_test_r2_epoch\n",
    "    }\n",
    "\n",
    "'''\n",
    "print(\"Best epochs based on test_loss:\")\n",
    "for fold, epochs in best_epochs.items():\n",
    "    print(f\"Fold {fold}: Epoch {epochs['best_test_loss_epoch']} (Test Loss)\")\n",
    "'''\n",
    "\n",
    "print(\"Best epochs based on test_r2:\")\n",
    "for fold, epochs in best_epochs.items():\n",
    "    print(f\"Fold {fold}: Epoch {epochs['best_test_r2_epoch']} (Test R-squared)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test name \n",
    "test_name = 'ts_conf_t100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epochs based on test_r2:\n",
      "Fold A: Epoch 60 (Test R-squared)\n",
      "Fold B: Epoch 60 (Test R-squared)\n",
      "Fold C: Epoch 55 (Test R-squared)\n",
      "Fold D: Epoch 57 (Test R-squared)\n",
      "Fold E: Epoch 59 (Test R-squared)\n"
     ]
    }
   ],
   "source": [
    "# Make a dico with best epochs and print them\n",
    "\n",
    "# Load the results from the pickle file\n",
    "with open('../models/results/conf_t/' + test_name + '.pkl', 'rb') as f:\n",
    "    dic = pickle.load(f)\n",
    "\n",
    "best_epochs = {}  # Dictionary to store the best epochs\n",
    "\n",
    "# Iterate through the folds\n",
    "for fold, fold_results in dic.items():\n",
    "    # Find the epoch with the maximum test_r2\n",
    "    best_test_r2_epoch = np.argmax(fold_results['test_r2']) + 1  # Adjust for 0-based index\n",
    "    \n",
    "    best_epochs[fold] = {\n",
    "        'best_test_r2_epoch': best_test_r2_epoch\n",
    "    }\n",
    "\n",
    "# Dictionary containing the best epochs for each fold\n",
    "best_epochs_dict = {} \n",
    "\n",
    "# Define the model names ('A', 'B', 'C', 'D', 'E')\n",
    "model_names = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "# Iterate through the model names\n",
    "for model_name in model_names:\n",
    "    # Get the best epochs for the current model\n",
    "    model_best_epochs = {\n",
    "        'best_test_r2_epoch': best_epochs[model_name]['best_test_r2_epoch']\n",
    "    }\n",
    "    \n",
    "    # Add the best epochs to the dictionary\n",
    "    best_epochs_dict[model_name] = model_best_epochs\n",
    "\n",
    "# Print the best epochs based on test_r2 in the specified format\n",
    "print(\"Best epochs based on test_r2:\")\n",
    "for model_name, epochs in best_epochs_dict.items():\n",
    "    print(f\"Fold {model_name}: Epoch {epochs['best_test_r2_epoch']} (Test R-squared)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best epoch checkpoint files in a folder\n",
    "\n",
    "# Path to the folder containing all checkpoints\n",
    "all_checkpoints_folder = '../models/checkpoints/conf_t/all'\n",
    "\n",
    "# Path to the folder where the best checkpoints will be copied \n",
    "best_checkpoints_folder = '../models/checkpoints/conf_t/'+test_name+'_best '\n",
    "\n",
    "# Iterate through the folds and models to copy the best checkpoints\n",
    "for fold, best_epoch in best_epochs.items():\n",
    "    # Model names \n",
    "    model_names = ['A', 'B', 'C', 'D', 'E']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Construct the checkpoint filename based on the provided structure\n",
    "        checkpoint_filename = f\"{test_name}_{model_name}_{fold}_{best_epoch}.pth\"\n",
    "        \n",
    "        # Source path of the best checkpoint file\n",
    "        source_path = os.path.join(all_checkpoints_folder, checkpoint_filename)\n",
    "        \n",
    "        # Destination path where the best checkpoint will be copied\n",
    "        destination_path = os.path.join(best_checkpoints_folder, checkpoint_filename)\n",
    "        \n",
    "        # Copy the best checkpoint to the 'best' folder\n",
    "        shutil.copy(source_path, destination_path)\n",
    "\n",
    "# Inform the user that the operation is completed\n",
    "print(\"Best checkpoints copied to 'best' folder.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
