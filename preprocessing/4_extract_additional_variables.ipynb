{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 14:39:46.835238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 14:39:46.895959: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import ee \n",
    "import ee_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Earth Engine LST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=xmIZobSB5d6d620M4MaIRIdtaGHHjVFt9jI7jKXBJqI&tc=9JOJkDYU8A7qWipHGVhlsTeL-Q74Rl7V7N155ggiEhE&cc=Nz8Sb4Wm13Q5ttocMmOGfk9wPDQG7Mqixr1D_knL3Fg>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=xmIZobSB5d6d620M4MaIRIdtaGHHjVFt9jI7jKXBJqI&tc=9JOJkDYU8A7qWipHGVhlsTeL-Q74Rl7V7N155ggiEhE&cc=Nz8Sb4Wm13Q5ttocMmOGfk9wPDQG7Mqixr1D_knL3Fg</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ADAPT THESE PARAMETERS ==========\n",
    "# To export to Google Drive, uncomment the next 2 lines\n",
    "EXPORT = ''\n",
    "BUCKET = None\n",
    "# export location parameters\n",
    "ERA5_EXPORT_FOLDER = ''\n",
    "CSV_PATH = '../data/dataset_viirs_only.csv'\n",
    "BANDS = ['mean_2m_air_temperature', 'minimum_2m_air_temperature', 'maximum_2m_air_temperature']\n",
    "# image export parameters\n",
    "PROJECTION = 'EPSG:3857'  # see https://epsg.io/3857\n",
    "SCALE = 30                # export resolution: 30m/px\n",
    "EXPORT_TILE_RADIUS = 3  # We only need the central values here\n",
    "CHUNK_SIZE = None    # set to a small number (<= 50) if Google Earth Engine reports memory errors; \n",
    "csv = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_images(\n",
    "        df: pd.DataFrame,\n",
    "        collection: ee.ImageCollection,\n",
    "        country: str,\n",
    "        year: int,\n",
    "        export_folder: str,\n",
    "        chunk_size = 1,\n",
    " ):\n",
    "    '''\n",
    "    Args\n",
    "    - df: pd.DataFrame, contains columns ['lat', 'lon', 'country', 'year']\n",
    "    - country: str, together with `year` determines the survey to export\n",
    "    - year: int, together with `country` determines the survey to export\n",
    "    - export_folder: str, name of folder for export\n",
    "    - chunk_size: int, optionally set a limit to the # of images exported per TFRecord file\n",
    "        - set to a small number (<= 50) if Google Earth Engine reports memory errors\n",
    "\n",
    "    Returns: dict, maps task name tuple (export_folder, country, year, chunk) to ee.batch.Task\n",
    "    '''\n",
    "\n",
    "    subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "    if chunk_size is None:\n",
    "        chunk_size = len(subset_df)\n",
    "    num_chunks = int(math.ceil(len(subset_df) / chunk_size))\n",
    "    tasks = {}\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_slice = slice(i * chunk_size, (i+1) * chunk_size - 1)  # df.loc[] is inclusive\n",
    "        fc = ee_utils.df_to_fc(subset_df.loc[chunk_slice, :])\n",
    "        for prev_year in range(year-4, year+1):\n",
    "            start_date, end_date = str(prev_year)+'-01-01',str(prev_year)+'-12-31'\n",
    "            roi = fc.geometry()\n",
    "            collection_max = collection.select(BANDS[2]).filterDate(start_date, end_date).filterBounds(roi)\n",
    "            collection_min = collection.select(BANDS[1]).filterDate(start_date, end_date).filterBounds(roi)\n",
    "            collection_ave = collection.select(BANDS[0]).filterDate(start_date, end_date).filterBounds(roi)\n",
    "            ave = collection_ave.median()\n",
    "            max = collection_max.max()\n",
    "            min = collection_min.min()\n",
    "            ave = ee_utils.add_latlon(ave)\n",
    "            max = ee_utils.add_latlon(max)\n",
    "            min = ee_utils.add_latlon(min)\n",
    "\n",
    "            fname = f'{country}_{year}_{prev_year}_{i:02d}'\n",
    "            tasks[(export_folder, country, prev_year, i)] = ee_utils.get_array_patches(\n",
    "                    img=ave, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
    "                    points=fc, export='drive',\n",
    "                    prefix=export_folder, fname=fname+'_ave',\n",
    "                    bucket=None), ee_utils.get_array_patches(\n",
    "                    img=min, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
    "                    points=fc, export='drive',\n",
    "                    prefix=export_folder, fname=fname+'_min',\n",
    "                    bucket=None), ee_utils.get_array_patches(\n",
    "                    img=max, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
    "                    points=fc, export='drive',\n",
    "                    prefix=export_folder, fname=fname+'_max',\n",
    "                    bucket=None)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = ee.ImageCollection(\"ECMWF/ERA5/MONTHLY\")\n",
    "dataset = pd.read_csv('../data/dataset_viirs_only.csv')\n",
    "dataset_ = list(dataset.groupby(['country', 'year']).groups.keys())\n",
    "tasks = {}\n",
    "for country, year in tqdm(dataset_):\n",
    "    print(country, year)\n",
    "    new_tasks = export_images(\n",
    "        df=dataset, collection=collection, country=country, year=year,\n",
    "        export_folder=ERA5_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = ['minimum_2m_air_temperature', 'maximum_2m_air_temperature','mean_2m_air_temperature']\n",
    "\n",
    "BANDS_ORDER = ['minimum_2m_air_temperature', 'maximum_2m_air_temperature','mean_2m_air_temperature']\n",
    "\n",
    "\n",
    "EXPORT_FOLDER = '../data/additional_data/temperature'\n",
    "PROCESSED_FOLDER = '../data/additional_data/temperature'\n",
    "def validate_and_split_tfrecords(\n",
    "        tfrecord_paths,\n",
    "        out_dir: str,\n",
    "        df: pd.DataFrame,\n",
    "        country,\n",
    "        year\n",
    "        ) -> None:\n",
    "    '''Validates and splits a list of exported TFRecord files (for a\n",
    "    given country-year survey) into individual TFrecords, one per cluster.\n",
    "\n",
    "    \"Validating\" a TFRecord comprises of 2 parts\n",
    "    1) verifying that it contains the required bands\n",
    "    2) verifying that its other features match the values from the dataset CSV\n",
    "\n",
    "    Args\n",
    "    - tfrecord_paths: list of str, paths to exported TFRecords files\n",
    "    - out_dir: str, path to dir to save processed individual TFRecords\n",
    "    - df: pd.DataFrame, index is sequential and starts at 0\n",
    "    '''\n",
    "    # Create an iterator over the TFRecords file. The iterator yields\n",
    "    # the binary representations of Example messages as strings.\n",
    "    options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "\n",
    "    # cast float64 => float32 and str => bytes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == object:  # pandas uses 'object' type for str\n",
    "            df[col] = df[col].astype(bytes)\n",
    "\n",
    "   \n",
    "    progbar = tqdm(total=len(df))\n",
    "\n",
    "    for tfrecord_path in tfrecord_paths:\n",
    "        iterator = tf.compat.v1.io.tf_record_iterator(tfrecord_path, options=options)\n",
    "        for record_str in iterator:\n",
    "            # parse into an actual Example message\n",
    "            ex = tf.train.Example.FromString(record_str)\n",
    "            feature_map = ex.features.feature\n",
    "            index = str(int(feature_map[\"cluster\"].float_list.value[0]))\n",
    "            # for band in REQUIRED_BANDS:\n",
    "            #     assert band in feature_map, f'Band \"{band}\" not in record {index} of {tfrecord_path}'\n",
    "            # serialize to string and write to file\n",
    "            out_path = os.path.join(out_dir, f'{index}'+\"_\"+tfrecord_path[-15:-12]+'.tfrecord.gz')  # all surveys have < 1e6 clusters\n",
    "            with tf.io.TFRecordWriter(out_path, options=options) as writer:\n",
    "                writer.write(ex.SerializeToString())\n",
    "\n",
    "            progbar.update(1)\n",
    "    progbar.close()\n",
    "    \n",
    "\n",
    "def process_dataset(csv_path: str, input_dir: str, processed_dir: str) -> None:\n",
    "    '''\n",
    "    Args\n",
    "    - csv_path: str, path to CSV of DHS or LSMS clusters\n",
    "    - input_dir: str, path to TFRecords exported from Google Earth Engine\n",
    "    - processed_dir: str, folder where to save processed TFRecords\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, float_precision='high', index_col=False)\n",
    "    surveys = list(df.groupby(['country', 'year']).groups.keys())  # (country, year) tuples\n",
    "   \n",
    "    # print(year, type(year))\n",
    "    for country, year in surveys:\n",
    "        if year == 2012: \n",
    "                year=2013  \n",
    "        \n",
    "        # Checking inside potential subfolders\n",
    "        for prev_year in range(year-4, year+1):\n",
    "            country_year = f'{country}_{year}_{prev_year}'\n",
    "            print('Processing:', country_year)\n",
    "            tfrecord_paths = glob(os.path.join(input_dir, country_year+'*.tfrecord.gz'))\n",
    "            tfrecord_paths += glob(os.path.join(input_dir, \"*\", country_year + '*.tfrecord.gz'))\n",
    "            tfrecord_paths += glob(os.path.join(input_dir, \"*\",\"*\", country_year + '*.tfrecord.gz'))\n",
    "\n",
    "            out_dir = os.path.join(processed_dir, country_year)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "            validate_and_split_tfrecords(\n",
    "            tfrecord_paths=tfrecord_paths, out_dir=out_dir, df=subset_df, country=country, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    csv_path='../data/dataset_viirs_only.csv',\n",
    "    input_dir=EXPORT_FOLDER,\n",
    "    processed_dir=PROCESSED_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "import torch\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV              = os.path.join( \"..\", \"data\", \"dataset_viirs_only.csv\" )\n",
    "RECORDS_DIR      = os.path.join( \"..\", \"data\", \"additional_data\", \"temperature\", \"\")\n",
    "TIF_DIR          = os.path.join( \"..\", \"data\", \"additional_data\", \"temperature\", \"\" )\n",
    "\n",
    "csv = pd.read_csv(CSV)\n",
    "records = dict()\n",
    "for year in csv.year.unique():\n",
    "    sub_year = csv[ csv.year == year ]\n",
    "    for prev_year in range(year-4, year+1):\n",
    "        records[year, prev_year] = dict()\n",
    "        for country in sub_year.country.unique():\n",
    "            sub_country = sub_year[ sub_year.country == country ].copy()\n",
    "            pattern = RECORDS_DIR+\"*\"+str(country)+\"_\"+str(year)+\"_\"+str(prev_year)+\"/*.tfrecord*\"\n",
    "            records[year,prev_year][country] = glob(pattern)\n",
    "\n",
    "def decompress_tfrecord(tfrecord_archive):\n",
    "    with gzip.open(tfrecord_archive, 'rb') as f_in:\n",
    "        # WITHOUT .GZ\n",
    "        with open(tfrecord_archive[:-3], 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    return tfrecord_archive[:-3]\n",
    "\n",
    "def tensor_to_string(data, variable):\n",
    "    filename = (data[variable].numpy())[0][0]\n",
    "    return str(filename).replace(\".\",\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTORMIN       = {\n",
    "                'cluster':\"float\",\n",
    "                'lat':\"float\", \n",
    "                \"lon\":\"float\",\n",
    "                'wealthpooled':\"float\",\n",
    "                'minimum_2m_air_temperature':'float',\n",
    "              } \n",
    "DESCRIPTORMAX       = {\n",
    "                'cluster':\"float\",\n",
    "                'lat':\"float\", \n",
    "                \"lon\":\"float\",\n",
    "                'wealthpooled':\"float\",\n",
    "                'maximum_2m_air_temperature':'float',\n",
    "              } \n",
    "DESCRIPTORAVE       = {\n",
    "                'cluster':\"float\",\n",
    "                'lat':\"float\", \n",
    "                \"lon\":\"float\",\n",
    "                'wealthpooled':\"float\",\n",
    "                'mean_2m_air_temperature':'float'\n",
    "              }  \n",
    "BANDNAMES = {'ave':'mean_2m_air_temperature', 'min':'minimum_2m_air_temperature','max':'maximum_2m_air_temperature'}\n",
    "\n",
    "def tfrecord_to_tif(data, filename, mins, maxs,band):\n",
    "    arrays = [] \n",
    "    new_arr = data[BANDNAMES[band]].numpy().reshape((7,7))\n",
    "    arrays.append(new_arr)\n",
    "    mins = min(mins, new_arr.min())\n",
    "    maxs = max(maxs, new_arr.max())\n",
    "\n",
    "    arr = np.swapaxes(np.array(arrays), 0, 2 )\n",
    "    tif_path = TIF_DIR + filename\n",
    "    tif = rasterio.open(tif_path, 'w', driver='GTiff',\n",
    "                            height = arr.shape[0], width = arr.shape[1],\n",
    "                            count=8, dtype=str(arr.dtype),\n",
    "                            crs='epsg:3857',\n",
    "                            transform=None)\n",
    "    tif.write(arr[:,:,0],1)\n",
    "    tif.close()\n",
    "\n",
    "    return mins, maxs\n",
    "\n",
    "def read_record(data,band):\n",
    "    new_arr = data[BANDNAMES[band]].numpy().reshape((7,7))\n",
    "    return new_arr[3,3]\n",
    "\n",
    "def map_row(row,band, val, year, country, cluster):\n",
    "    if int(row.year)==int(year) and row.country==country and int(row.cluster) == int(cluster):\n",
    "        row[str(int(prev_year))+'_'+band] = val\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>cluster</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>households</th>\n",
       "      <th>wealthpooled</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>fold</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.1014</td>\n",
       "      <td>14.1407</td>\n",
       "      <td>26</td>\n",
       "      <td>-1.132639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>[[662.8333333333334, 0, 1648], [681.9166666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.6635</td>\n",
       "      <td>20.3770</td>\n",
       "      <td>26</td>\n",
       "      <td>0.669843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E</td>\n",
       "      <td>[[1048.25, 0, 2346], [1021.0, 0, 2415], [1059....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>-8.9289</td>\n",
       "      <td>13.2995</td>\n",
       "      <td>10</td>\n",
       "      <td>1.515591</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>[[555.0833333333334, 0, 2752], [242.9166666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>-14.2876</td>\n",
       "      <td>17.6217</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.559135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>[[980.75, 0, 2385], [853.9166666666666, 0, 234...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>-14.2110</td>\n",
       "      <td>13.5463</td>\n",
       "      <td>26</td>\n",
       "      <td>-1.186118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>[[656.25, 0, 2028], [491.75, 0, 2191], [634.75...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  year  cluster      lat      lon  households  wealthpooled  \\\n",
       "0  angola  2015        1 -12.1014  14.1407          26     -1.132639   \n",
       "1  angola  2015        2  -9.6635  20.3770          26      0.669843   \n",
       "2  angola  2015        3  -8.9289  13.2995          10      1.515591   \n",
       "3  angola  2015        4 -14.2876  17.6217          26     -0.559135   \n",
       "4  angola  2015        5 -14.2110  13.5463          26     -1.186118   \n",
       "\n",
       "   urban_rural fold                                      precipitation  \n",
       "0          0.0    D  [[662.8333333333334, 0, 1648], [681.9166666666...  \n",
       "1          1.0    E  [[1048.25, 0, 2346], [1021.0, 0, 2415], [1059....  \n",
       "2          1.0    D  [[555.0833333333334, 0, 2752], [242.9166666666...  \n",
       "3          0.0    B  [[980.75, 0, 2385], [853.9166666666666, 0, 234...  \n",
       "4          0.0    B  [[656.25, 0, 2028], [491.75, 0, 2191], [634.75...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_pickle('../data/dataset_precipitation.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 2011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m tfrecord_archive \u001b[39min\u001b[39;00m records[year,prev_year][country]:\n\u001b[1;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m tfrecord_archive[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.gz\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m         tfrecord \u001b[39m=\u001b[39m decompress_tfrecord(tfrecord_archive\u001b[39m=\u001b[39;49mtfrecord_archive)\n\u001b[1;32m      7\u001b[0m         tfrecord \u001b[39m=\u001b[39m tfrecord_archive[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mdecompress_tfrecord\u001b[0;34m(tfrecord_archive)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecompress_tfrecord\u001b[39m(tfrecord_archive):\n\u001b[1;32m     17\u001b[0m     \u001b[39mwith\u001b[39;00m gzip\u001b[39m.\u001b[39mopen(tfrecord_archive, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f_in:\n\u001b[1;32m     18\u001b[0m         \u001b[39m# WITHOUT .GZ\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(tfrecord_archive[:\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f_out:\n\u001b[1;32m     20\u001b[0m             shutil\u001b[39m.\u001b[39mcopyfileobj(f_in, f_out)\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m tfrecord_archive[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year, prev_year in records:\n",
    "    print(year, prev_year)\n",
    "    for country in records[year, prev_year]:\n",
    "        for tfrecord_archive in records[year,prev_year][country]:\n",
    "            if tfrecord_archive[-3:] == '.gz':\n",
    "                tfrecord = decompress_tfrecord(tfrecord_archive=tfrecord_archive)\n",
    "                tfrecord = tfrecord_archive[:-3]\n",
    "            else:\n",
    "                tfrecord = tfrecord_archive\n",
    "            band = tfrecord[-12:-9]\n",
    "            if band == 'ave':\n",
    "                descriptor = DESCRIPTORAVE\n",
    "            if band == 'min':\n",
    "                descriptor = DESCRIPTORMIN\n",
    "            if band == 'max':\n",
    "                descriptor = DESCRIPTORMAX\n",
    "            dataset = TFRecordDataset(tfrecord, index_path=None, description=descriptor)\n",
    "            loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "            iterator = iter(loader)\n",
    "            while (data := next(iterator, None)) is not None:\n",
    "                # filename = str(country)+\"_\"+str(year)+\"_\"+str(prev_year)+\"/\"+tensor_to_string(data, \"cluster\")[:-1]+\"_\"+band+\".tif\"\n",
    "                # mins[band], maxs[band] = tfrecord_to_tif(data, filename, mins[band], maxs[band], band)\n",
    "                val = read_record(data,band)\n",
    "                if str(int(prev_year))+'_'+band not in df.columns:\n",
    "                    df[str(int(prev_year))+'_'+band] = ''\n",
    "                cluster =tensor_to_string(data, \"cluster\")[:-1]\n",
    "                df = df.apply(lambda row: map_row(row,band, val, year ,country,cluster),axis=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14386"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle('../data/dataset_additional.pkl')\n",
    "len(dataset[dataset['temperature']==''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tmp(row):\n",
    "    vector = []\n",
    "    for year in range(int(row.year)-4,int(row.year)+1):\n",
    "        bands = ['ave', 'min', 'max']\n",
    "        val = [0.,0.,0.]\n",
    "        for i in range(len(bands)):\n",
    "            tif = os.path.join( os.path.join('../data', 'additional_data','temperature'), str(row.country)+\"_\"+str(int(row.year))+\"_\"+str(int(year)), str(int(row.cluster))+\"_\"+bands[i]+\".tif\")\n",
    "            with rio.open(tif) as src: \n",
    "                # We extract the central value\n",
    "                for value in src.sample([(3, 3)]): \n",
    "                    val[i] = value[0]\n",
    "                src.close()\n",
    "            vector.append(val)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('../data/dataset_additional.pkl')\n",
    "dataset.temperature = dataset.apply(map_tmp, axis=1)\n",
    "dataset.to_pickle('../data/dataset_additional.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FOA WAPOR PCP (From CHIRPS catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/dataset_viirs_only.csv')\n",
    "PATH = os.path.join('../data', 'additional_data','precipitation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()\n",
    "dataset.to_csv('data/dataset_additional.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"precipitation\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN = 1e6\n",
    "MAX = -1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_island_coordinates(x,y):\n",
    "    '''return the closest valid points when dealing with islands due to coarse tif resolution'''\n",
    "    # Tanzania\n",
    "    if int(y)==-5 or int(y)==-6 and int(x)==39:\n",
    "        return 39.29, -5.98\n",
    "    # Sierra Leone \n",
    "    if (int(x)==-13 or int(x)==-12) and (int(y)==8 or int(y)==7):\n",
    "        return -12.7, 7.8\n",
    "    # Senegal \n",
    "    if int(x) in [-12,-13,-16,-17] and int(y) in [12,13,14,15,16]:\n",
    "        return x-1, y\n",
    "    # Mozambique \n",
    "    if (int(x), int(y)) in [(32,-25),(40,-12)]:\n",
    "        return int(x), int(y)\n",
    "    if (int(x), int(y))== (34, -19):\n",
    "        return 34.80, -19.80\n",
    "    # Madagascar\n",
    "    if (int(x)==43 and int(y)==-23):\n",
    "        return x+1, y\n",
    "    if (int(x) in (48,49) and int(y) in (-12,-13)):\n",
    "        return x, y-2 \n",
    "    # Guinea \n",
    "    if int(x)==-13 and int(y)==9:\n",
    "        return x+0.5, y+0.5\n",
    "    # Cote d'Ivoir\n",
    "    if int(x)==-6 and int(y)==4:\n",
    "        return x+0.5, y+0.5\n",
    "    # Benin\n",
    "    if int(x) in (1,2) and int(y)==6:\n",
    "        return x, y+0.5\n",
    "    # Angola\n",
    "    if int(x)==-13 and int(y) in (-8,-12):\n",
    "        return x+0.5, y\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>cluster</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>households</th>\n",
       "      <th>wealthpooled</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>fold</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.1014</td>\n",
       "      <td>14.1407</td>\n",
       "      <td>26</td>\n",
       "      <td>-1.132639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>[[662.8333333333334, 0, 1648], [681.9166666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.6635</td>\n",
       "      <td>20.3770</td>\n",
       "      <td>26</td>\n",
       "      <td>0.669843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E</td>\n",
       "      <td>[[1048.25, 0, 2346], [1021.0, 0, 2415], [1059....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>-8.9289</td>\n",
       "      <td>13.2995</td>\n",
       "      <td>10</td>\n",
       "      <td>1.515591</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>[[555.0833333333334, 0, 2752], [242.9166666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>-14.2876</td>\n",
       "      <td>17.6217</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.559135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>[[980.75, 0, 2385], [853.9166666666666, 0, 234...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angola</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>-14.2110</td>\n",
       "      <td>13.5463</td>\n",
       "      <td>26</td>\n",
       "      <td>-1.186118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>[[656.25, 0, 2028], [491.75, 0, 2191], [634.75...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  year  cluster      lat      lon  households  wealthpooled  \\\n",
       "0  angola  2015        1 -12.1014  14.1407          26     -1.132639   \n",
       "1  angola  2015        2  -9.6635  20.3770          26      0.669843   \n",
       "2  angola  2015        3  -8.9289  13.2995          10      1.515591   \n",
       "3  angola  2015        4 -14.2876  17.6217          26     -0.559135   \n",
       "4  angola  2015        5 -14.2110  13.5463          26     -1.186118   \n",
       "\n",
       "   urban_rural fold                                      precipitation  \n",
       "0          0.0    D  [[662.8333333333334, 0, 1648], [681.9166666666...  \n",
       "1          1.0    E  [[1048.25, 0, 2346], [1021.0, 0, 2415], [1059....  \n",
       "2          1.0    D  [[555.0833333333334, 0, 2752], [242.9166666666...  \n",
       "3          0.0    B  [[980.75, 0, 2385], [853.9166666666666, 0, 234...  \n",
       "4          0.0    B  [[656.25, 0, 2028], [491.75, 0, 2191], [634.75...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for year in dataset.year.unique():\n",
    "    df = dataset[dataset.year==year]\n",
    "    for country in df.country.unique():\n",
    "        df_country = df[df.country==country]\n",
    "        for cluster in df_country.cluster.unique():\n",
    "            vector = []\n",
    "            row = df_country.loc[(df['cluster'] == cluster)]\n",
    "            x = float(row.at[row.index[0],'lon'])\n",
    "            y = float(row.at[row.index[0],'lat'])\n",
    "            x, y = correct_island_coordinates(x,y)\n",
    "            for prev_year in range(year-4,year+1):\n",
    "                monthly_tifs = glob.glob(os.path.join(PATH, str(prev_year)+\"*.tif\"))\n",
    "                min_row = 1e6\n",
    "                max_row = -1e5   \n",
    "                ave_row = 0\n",
    "                for tif in monthly_tifs:\n",
    "                    with rio.open(tif) as src: \n",
    "                        for val in src.sample([(x, y)]): \n",
    "                            # THE ORIGINAL RASTERS HAVE TO COARSE RESOLUTION TO CAPTURE SMALL ISLANDS AS VALID COORDINATES\n",
    "                            # WE TAKE THE THE CLOSEST COASTAL POINT IN THIS CASE \n",
    "\n",
    "                            max_row = max(val[0], max_row)\n",
    "                            min_row = min(val[0], min_row)\n",
    "                            ave_row += val[0]\n",
    "                            MIN = min(val[0], MIN)\n",
    "                            MAX = max(val[0], MAX)\n",
    "                ave_row /= len(monthly_tifs)\n",
    "                vector.append([ave_row, min_row, max_row])\n",
    "            dataset.at[row.index[0],'precipitation'] = vector\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('../data/dataset_precipitation.csv',index=False)\n",
    "dataset.to_pickle('../data/dataset_precipitation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle('../data/dataset_precipitation.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
