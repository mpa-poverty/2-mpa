{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Pre-requisites\" data-toc-modified-id=\"Pre-requisites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Pre-requisites</a></span></li><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Instructions</a></span></li><li><span><a href=\"#Imports-and-Constants\" data-toc-modified-id=\"Imports-and-Constants-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports and Constants</a></span></li><li><span><a href=\"#Validate-and-Split-Exported-TFRecords\" data-toc-modified-id=\"Validate-and-Split-Exported-TFRecords-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Validate and Split Exported TFRecords</a></span></li><li><span><a href=\"#Calculate-Mean-and-Std-Dev-for-Each-Band\" data-toc-modified-id=\"Calculate-Mean-and-Std-Dev-for-Each-Band-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculate Mean and Std-Dev for Each Band</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Go through the [`preprocessing/0_export_tfrecords.ipynb`](./0_export_tfrecords.ipynb) notebook.\n",
    "\n",
    "Before running this notebook, you should have the following structure under the `data/` directory:\n",
    "\n",
    "```\n",
    "data/\n",
    "    dhs_tfrecords_raw/\n",
    "        angola_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2015_XX.tfrecord.gz\n",
    "    dhsnl_tfrecords_raw/\n",
    "        angola_2010_00.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2016_XX.tfrecord.gz\n",
    "    lsms_tfrecords_raw/\n",
    "        ethiopia_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        uganda_2013_XX.tfrecord.gz\n",
    "```\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook processes the exported TFRecords as follows:\n",
    "1. Verifies that the fields in the TFRecords match the original CSV files.\n",
    "2. Splits each monolithic TFRecord file exported from Google Earth Engine into one file per record.\n",
    "\n",
    "After running this notebook, you should have three new folders (`dhs_tfrecords`, `dhsnl_tfrecords`, and `lsms_tfrecords`) under `data/`:\n",
    "\n",
    "```\n",
    "data/\n",
    "    dhs_tfrecords/\n",
    "        angola_2011/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00229.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2015/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00399.tfrecord.gz\n",
    "    dhsnl_tfrecords/\n",
    "        angola_2010/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            07734.tfrecord.gz\n",
    "        zimbabwe_2016/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            03584.tfrecord.gz\n",
    "    lsms_tfrecords/\n",
    "        ethiopia_2011/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00326.tfrecord.gz\n",
    "        uganda_2013/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00164.tfrecord.gz\n",
    "```\n",
    "\n",
    "This notebook also calculates the mean and standard deviation of each band across each of the 3 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/media/matthieu/LaCie/2-mpa\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change directory to repo root, and verify\n",
    "# %cd '../'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from batchers import batcher, tfrecord_paths_utils\n",
    "from helper import (\n",
    "    analyze_tfrecord_batch,\n",
    "    per_band_mean_std,\n",
    "    print_analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = [\n",
    "    'BLUE', 'GREEN', 'LAT', 'LON', 'NIGHTLIGHTS', 'NIR', 'RED',\n",
    "    'SWIR1', 'SWIR2', 'TEMP1']\n",
    "\n",
    "BANDS_ORDER = [\n",
    "    'BLUE', 'GREEN', 'RED', 'SWIR1', 'SWIR2', 'TEMP1', 'NIR',\n",
    "    'DMSP', 'VIIRS']\n",
    "\n",
    "\n",
    "DHSNL_EXPORT_FOLDER = 'data/landsat_7_raw'\n",
    "DHSNL_PROCESSED_FOLDER = 'data/landsat_7_records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_filename(row):\n",
    "#     row.filename=str(row.lat).replace(\".\",\"\")+\"_\"+str(row.lon).replace(\".\",\"\")+\".tif\"\n",
    "#     return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV= os.path.join( \".\",\"data\", \"wealth_index.csv\" )\n",
    "csv = pd.read_csv(CSV)\n",
    "# csv = csv.apply(map_filename, axis=1)\n",
    "# csv.to_csv(CSV,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_string(data, variable):\n",
    "    filename = (data[variable].numpy())[0]\n",
    "    return \"\".join([chr(item) for item in filename]).replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(csv_path: str, input_dir: str, processed_dir: str) -> None:\n",
    "    '''\n",
    "    Args\n",
    "    - csv_path: str, path to CSV of DHS or LSMS clusters\n",
    "    - input_dir: str, path to TFRecords exported from Google Earth Engine\n",
    "    - processed_dir: str, folder where to save processed TFRecords\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, float_precision='high', index_col=False)\n",
    "    surveys = list(df.groupby(['country', 'year']).groups.keys())  # (country, year) tuples\n",
    "\n",
    "    for country, year in surveys:\n",
    "        country_year = f'{country}_{year}'\n",
    "        print('Processing:', country_year)\n",
    "\n",
    "        tfrecord_paths = glob(os.path.join(input_dir, country_year + '*'))\n",
    "        out_dir = os.path.join(processed_dir, country_year)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "        validate_and_split_tfrecords(\n",
    "            tfrecord_paths=tfrecord_paths, out_dir=out_dir, df=subset_df, country=country, year=year)\n",
    "\n",
    "\n",
    "def validate_and_split_tfrecords(\n",
    "        tfrecord_paths: Iterable[str],\n",
    "        out_dir: str,\n",
    "        df: pd.DataFrame,\n",
    "        country,\n",
    "        year\n",
    "        ) -> None:\n",
    "    '''Validates and splits a list of exported TFRecord files (for a\n",
    "    given country-year survey) into individual TFrecords, one per cluster.\n",
    "\n",
    "    \"Validating\" a TFRecord comprises of 2 parts\n",
    "    1) verifying that it contains the required bands\n",
    "    2) verifying that its other features match the values from the dataset CSV\n",
    "\n",
    "    Args\n",
    "    - tfrecord_paths: list of str, paths to exported TFRecords files\n",
    "    - out_dir: str, path to dir to save processed individual TFRecords\n",
    "    - df: pd.DataFrame, index is sequential and starts at 0\n",
    "    '''\n",
    "    # Create an iterator over the TFRecords file. The iterator yields\n",
    "    # the binary representations of Example messages as strings.\n",
    "    options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "\n",
    "    # cast float64 => float32 and str => bytes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == object:  # pandas uses 'object' type for str\n",
    "            df[col] = df[col].astype(bytes)\n",
    "\n",
    "   \n",
    "    progbar = tqdm(total=len(df))\n",
    "\n",
    "    for tfrecord_path in tfrecord_paths:\n",
    "        iterator = tf.compat.v1.io.tf_record_iterator(tfrecord_path, options=options)   \n",
    "        for record_str in iterator:\n",
    "            # parse into an actual Example message\n",
    "            ex = tf.train.Example.FromString(record_str)\n",
    "            feature_map = ex.features.feature\n",
    "            index = str(country)+\"_\"+str(year)+\"_\"+str(feature_map[\"wealthpooled\"].float_list.value[0]).replace('.','')[:5]\n",
    "\n",
    "            for band in REQUIRED_BANDS:\n",
    "                assert band in feature_map, f'Band \"{band}\" not in record {index} of {tfrecord_path}'\n",
    "            # serialize to string and write to file\n",
    "            out_path = os.path.join(out_dir, f'{index}.tfrecord.gz')  # all surveys have < 1e6 clusters\n",
    "            with tf.io.TFRecordWriter(out_path, options=options) as writer:\n",
    "                writer.write(ex.SerializeToString())\n",
    "\n",
    "            progbar.update(1)\n",
    "    progbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('data/wealth_index.csv')\n",
    "csv.drop([\"households\", \"wealthpooled\",\"geometry\",\"filename\",\"bounding_box\"], axis=1, inplace=True)\n",
    "csv.to_csv('data/dhsnl_locs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: angola_2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 229/229 [00:23<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: angola_2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m process_dataset(\n\u001b[1;32m      2\u001b[0m     csv_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata/dhsnl_locs.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     input_dir\u001b[39m=\u001b[39;49mDHSNL_EXPORT_FOLDER,\n\u001b[1;32m      4\u001b[0m     processed_dir\u001b[39m=\u001b[39;49mDHSNL_PROCESSED_FOLDER)\n",
      "Cell \u001b[0;32mIn[39], line 19\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(csv_path, input_dir, processed_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m os\u001b[39m.\u001b[39mmakedirs(out_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m subset_df \u001b[39m=\u001b[39m df[(df[\u001b[39m'\u001b[39m\u001b[39mcountry\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m country) \u001b[39m&\u001b[39m (df[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m year)]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m validate_and_split_tfrecords(\n\u001b[1;32m     20\u001b[0m     tfrecord_paths\u001b[39m=\u001b[39;49mtfrecord_paths, out_dir\u001b[39m=\u001b[39;49mout_dir, df\u001b[39m=\u001b[39;49msubset_df, country\u001b[39m=\u001b[39;49mcountry, year\u001b[39m=\u001b[39;49myear)\n",
      "Cell \u001b[0;32mIn[39], line 69\u001b[0m, in \u001b[0;36mvalidate_and_split_tfrecords\u001b[0;34m(tfrecord_paths, out_dir, df, country, year)\u001b[0m\n\u001b[1;32m     67\u001b[0m         out_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(out_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m.tfrecord.gz\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# all surveys have < 1e6 clusters\u001b[39;00m\n\u001b[1;32m     68\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mTFRecordWriter(out_path, options\u001b[39m=\u001b[39moptions) \u001b[39mas\u001b[39;00m writer:\n\u001b[0;32m---> 69\u001b[0m             writer\u001b[39m.\u001b[39;49mwrite(ex\u001b[39m.\u001b[39;49mSerializeToString())\n\u001b[1;32m     71\u001b[0m         progbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m progbar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/tensorflow/python/lib/io/tf_record.py:309\u001b[0m, in \u001b[0;36mTFRecordWriter.write\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, record):\n\u001b[1;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Write a string record to the file.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m    record: str\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m   \u001b[39msuper\u001b[39;49m(TFRecordWriter, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite(record)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_dataset(\n",
    "    csv_path='data/dhsnl_locs.csv',\n",
    "    input_dir=DHSNL_EXPORT_FOLDER,\n",
    "    processed_dir=DHSNL_PROCESSED_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mean and Std-Dev for Each Band\n",
    "\n",
    "The means and standard deviations calculated here are saved as constants in `batchers/dataset_constants.py` for `_MEANS_DHS`, `_STD_DEVS_DHS`, `_MEANS_LSMS`, and `_STD_DEVS_LSMS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(tfrecord_paths):\n",
    "    '''Calculates and prints the per-band means and std-devs'''\n",
    "    iter_init, batch_op = batcher.Batcher(\n",
    "        tfrecord_files=tfrecord_paths,\n",
    "        label_name=None,\n",
    "        ls_bands='ms',\n",
    "        nl_band='merge',\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        augment=False,\n",
    "        clipneg=False,\n",
    "        normalize=None).get_batch()\n",
    "\n",
    "    stats = analyze_tfrecord_batch(\n",
    "        iter_init, batch_op, total_num_images=len(tfrecord_paths),\n",
    "        nbands=len(BANDS_ORDER), k=10)\n",
    "    means, stds = per_band_mean_std(stats=stats, band_order=BANDS_ORDER)\n",
    "\n",
    "    print('Means:')\n",
    "    pprint(means)\n",
    "    print()\n",
    "\n",
    "    print('Std Devs:')\n",
    "    pprint(stds)\n",
    "\n",
    "    print('\\n========== Additional Per-band Statistics ==========\\n')\n",
    "    print_analysis_results(stats, BANDS_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecord_paths_utils.dhsnl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Processed 0 images.\n",
      "Time per batch - mean: nans, std: nans\n",
      "Time to process each batch - mean: nans, std: nans\n",
      "Total time: 0.163s, Num batches: 0\n",
      "Means:\n",
      "{'BLUE': nan,\n",
      " 'DMSP': nan,\n",
      " 'GREEN': nan,\n",
      " 'NIR': nan,\n",
      " 'RED': nan,\n",
      " 'SWIR1': nan,\n",
      " 'SWIR2': nan,\n",
      " 'TEMP1': nan,\n",
      " 'VIIRS': nan}\n",
      "\n",
      "Std Devs:\n",
      "{'BLUE': nan,\n",
      " 'DMSP': nan,\n",
      " 'GREEN': nan,\n",
      " 'NIR': nan,\n",
      " 'RED': nan,\n",
      " 'SWIR1': nan,\n",
      " 'SWIR2': nan,\n",
      " 'TEMP1': nan,\n",
      " 'VIIRS': nan}\n",
      "\n",
      "========== Additional Per-band Statistics ==========\n",
      "\n",
      "Statistics including bad pixels\n",
      "Band BLUE     - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band GREEN    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band RED      - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band SWIR1    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band SWIR2    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band TEMP1    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band NIR      - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band DMSP     - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band VIIRS    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "\n",
      "Statistics ignoring any 0s and negative values\n",
      "Band BLUE     - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band GREEN    - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band RED      - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band SWIR1    - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band SWIR2    - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band TEMP1    - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band NIR      - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band DMSP     - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "Band VIIRS    - mean:        nan, std:       nan, min:         inf, max:    0.000000, mean_nz: nan\n",
      "\n",
      "Statistics excluding the bad pixels\n",
      "Band BLUE     - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band GREEN    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band RED      - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band SWIR1    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band SWIR2    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band TEMP1    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band NIR      - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band DMSP     - mean:        nan, std:       nan, min:         inf, max:    0.000000\n",
      "Band VIIRS    - mean:        nan, std:       nan, min:         inf, max:    0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 14:34:34.955014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22211 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2023-04-26 14:34:34.958775: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:154: RuntimeWarning: invalid value encountered in divide\n",
      "  means = sums / float(num_total_pixels)\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:155: RuntimeWarning: invalid value encountered in divide\n",
      "  stds = np.sqrt(sum_sqs/float(num_total_pixels) - means**2)\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  means = sums / float(total_pixels_per_band)\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:185: RuntimeWarning: invalid value encountered in divide\n",
      "  stds = np.sqrt(sum_sqs/float(total_pixels_per_band) - means**2)\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  means = sums / nz_pixels\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:193: RuntimeWarning: invalid value encountered in divide\n",
      "  stds = np.sqrt(sum_sqs/nz_pixels - means**2)\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:194: RuntimeWarning: invalid value encountered in divide\n",
      "  avg_nz_pixels = nz_pixels.astype(np.float32) / images_count\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:202: RuntimeWarning: invalid value encountered in divide\n",
      "  means = sums / float(num_total_pixels)\n",
      "/media/matthieu/LaCie/2-mpa/preprocessing/helper.py:203: RuntimeWarning: invalid value encountered in divide\n",
      "  stds = np.sqrt(sum_sqs/float(num_total_pixels) - means**2)\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "calculate_mean_std(tfrecord_paths_utils.dhsnl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "01ed5c42258d104453582e2fee2faf5d01150c2a161fd6cc7123c0fdfe444c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
