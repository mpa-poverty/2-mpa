{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee11b763",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065c3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df09711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from run_experiment import run_experiment\n",
    "from utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb28d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = 'configs/resnet18_ms_e2e_l7_1e2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa6963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 36% |  2% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 49% |  3% |\n"
     ]
    }
   ],
   "source": [
    "utils.free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230accfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold A has begun.\n",
      "Epoch: 1 | train_loss: 0.1031 | train_r2: -1.5278 | test_loss: 0.0459 | test_r2: -0.7909\n",
      "Epoch: 2 | train_loss: 0.0632 | train_r2: -0.5489 | test_loss: 0.0453 | test_r2: -0.7660\n",
      "Epoch: 3 | train_loss: 0.0422 | train_r2: -0.0354 | test_loss: 0.0337 | test_r2: -0.3167\n",
      "Epoch: 4 | train_loss: 0.0302 | train_r2: 0.2597 | test_loss: 0.0278 | test_r2: -0.0867\n",
      "Epoch: 5 | train_loss: 0.0223 | train_r2: 0.4540 | test_loss: 0.0309 | test_r2: -0.2040\n",
      "Epoch: 6 | train_loss: 0.0294 | train_r2: 0.2792 | test_loss: 0.0300 | test_r2: -0.1719\n",
      "Epoch: 7 | train_loss: 0.0249 | train_r2: 0.3898 | test_loss: 0.0271 | test_r2: -0.0587\n",
      "Epoch: 8 | train_loss: 0.0224 | train_r2: 0.4516 | test_loss: 0.0264 | test_r2: -0.0291\n",
      "Epoch: 9 | train_loss: 0.0211 | train_r2: 0.4827 | test_loss: 0.0634 | test_r2: -1.4759\n",
      "Epoch: 10 | train_loss: 0.0211 | train_r2: 0.4820 | test_loss: 0.1146 | test_r2: -3.4708\n",
      "Epoch: 11 | train_loss: 0.0187 | train_r2: 0.5416 | test_loss: 0.1927 | test_r2: -6.5217\n",
      "Epoch: 12 | train_loss: 0.0175 | train_r2: 0.5703 | test_loss: 0.3995 | test_r2: -14.5887\n",
      "Epoch: 13 | train_loss: 0.0161 | train_r2: 0.6042 | test_loss: 0.7469 | test_r2: -28.1448\n",
      "Epoch: 14 | train_loss: 0.0150 | train_r2: 0.6318 | test_loss: 0.7517 | test_r2: -28.3348\n",
      "Epoch: 15 | train_loss: 0.0139 | train_r2: 0.6596 | test_loss: 0.3689 | test_r2: -13.3939\n",
      "Epoch: 16 | train_loss: 0.0122 | train_r2: 0.7017 | test_loss: 0.1281 | test_r2: -3.9990\n",
      "Epoch: 17 | train_loss: 0.0103 | train_r2: 0.7466 | test_loss: 0.2374 | test_r2: -8.2626\n",
      "Epoch: 18 | train_loss: 0.0131 | train_r2: 0.6792 | test_loss: 0.0286 | test_r2: -0.1170\n",
      "Epoch: 19 | train_loss: 0.0280 | train_r2: 0.3133 | test_loss: 0.2010 | test_r2: -6.8450\n",
      "Epoch: 20 | train_loss: 0.0147 | train_r2: 0.6393 | test_loss: 0.2236 | test_r2: -7.7263\n",
      "Epoch: 21 | train_loss: 0.0152 | train_r2: 0.6271 | test_loss: 0.2394 | test_r2: -8.3436\n",
      "Epoch: 22 | train_loss: 0.0120 | train_r2: 0.7057 | test_loss: 0.2449 | test_r2: -8.5560\n",
      "Epoch: 23 | train_loss: 0.0103 | train_r2: 0.7467 | test_loss: 0.2509 | test_r2: -8.7915\n",
      "Epoch: 24 | train_loss: 0.0104 | train_r2: 0.7457 | test_loss: 0.2554 | test_r2: -8.9673\n",
      "Epoch: 25 | train_loss: 0.0089 | train_r2: 0.7816 | test_loss: 0.2418 | test_r2: -8.4360\n",
      "Epoch: 26 | train_loss: 0.0086 | train_r2: 0.7894 | test_loss: 0.2084 | test_r2: -7.1308\n",
      "Epoch: 27 | train_loss: 0.0082 | train_r2: 0.7988 | test_loss: 0.1694 | test_r2: -5.6102\n",
      "Epoch: 28 | train_loss: 0.0081 | train_r2: 0.8004 | test_loss: 0.1304 | test_r2: -4.0906\n",
      "Epoch: 29 | train_loss: 0.0084 | train_r2: 0.7942 | test_loss: 0.0953 | test_r2: -2.7208\n",
      "Epoch: 30 | train_loss: 0.0094 | train_r2: 0.7694 | test_loss: 0.0666 | test_r2: -1.5971\n",
      "Epoch: 31 | train_loss: 0.0077 | train_r2: 0.8105 | test_loss: 0.0526 | test_r2: -1.0515\n",
      "Epoch: 32 | train_loss: 0.0092 | train_r2: 0.7738 | test_loss: 0.0436 | test_r2: -0.7026\n",
      "Epoch: 33 | train_loss: 0.0095 | train_r2: 0.7675 | test_loss: 0.0375 | test_r2: -0.4623\n",
      "Epoch: 34 | train_loss: 0.0077 | train_r2: 0.8108 | test_loss: 0.0336 | test_r2: -0.3108\n",
      "Epoch: 35 | train_loss: 0.0076 | train_r2: 0.8146 | test_loss: 0.0321 | test_r2: -0.2521\n",
      "Epoch: 36 | train_loss: 0.0075 | train_r2: 0.8163 | test_loss: 0.0317 | test_r2: -0.2387\n",
      "Epoch: 37 | train_loss: 0.0071 | train_r2: 0.8258 | test_loss: 0.0316 | test_r2: -0.2320\n",
      "Epoch: 38 | train_loss: 0.0086 | train_r2: 0.7898 | test_loss: 0.0310 | test_r2: -0.2094\n",
      "Epoch: 39 | train_loss: 0.0085 | train_r2: 0.7919 | test_loss: 0.0300 | test_r2: -0.1706\n",
      "Epoch: 40 | train_loss: 0.0082 | train_r2: 0.7998 | test_loss: 0.0288 | test_r2: -0.1244\n",
      "Epoch: 41 | train_loss: 0.0065 | train_r2: 0.8406 | test_loss: 0.0276 | test_r2: -0.0770\n",
      "Epoch: 42 | train_loss: 0.0079 | train_r2: 0.8064 | test_loss: 0.0263 | test_r2: -0.0273\n",
      "Epoch: 43 | train_loss: 0.0080 | train_r2: 0.8051 | test_loss: 0.0254 | test_r2: 0.0107\n",
      "Epoch: 44 | train_loss: 0.0080 | train_r2: 0.8045 | test_loss: 0.0248 | test_r2: 0.0329\n",
      "Epoch: 45 | train_loss: 0.0081 | train_r2: 0.8003 | test_loss: 0.0246 | test_r2: 0.0399\n",
      "Epoch: 46 | train_loss: 0.0078 | train_r2: 0.8090 | test_loss: 0.0247 | test_r2: 0.0350\n",
      "Epoch: 47 | train_loss: 0.0079 | train_r2: 0.8071 | test_loss: 0.0250 | test_r2: 0.0225\n",
      "Epoch: 48 | train_loss: 0.0080 | train_r2: 0.8035 | test_loss: 0.0255 | test_r2: 0.0043\n",
      "Epoch: 49 | train_loss: 0.0078 | train_r2: 0.8076 | test_loss: 0.0261 | test_r2: -0.0172\n",
      "Epoch: 50 | train_loss: 0.0081 | train_r2: 0.8020 | test_loss: 0.0266 | test_r2: -0.0395\n",
      "Epoch: 51 | train_loss: 0.0079 | train_r2: 0.8056 | test_loss: 0.0272 | test_r2: -0.0629\n",
      "Epoch: 52 | train_loss: 0.0078 | train_r2: 0.8075 | test_loss: 0.0278 | test_r2: -0.0851\n",
      "Epoch: 53 | train_loss: 0.0080 | train_r2: 0.8047 | test_loss: 0.0283 | test_r2: -0.1056\n",
      "Epoch: 54 | train_loss: 0.0077 | train_r2: 0.8122 | test_loss: 0.0289 | test_r2: -0.1267\n",
      "Epoch: 55 | train_loss: 0.0066 | train_r2: 0.8370 | test_loss: 0.0292 | test_r2: -0.1401\n",
      "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 22.02 GiB already allocated; 20.62 MiB free; 22.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 22.02 GiB already allocated; 20.62 MiB free; 22.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m run_experiment(config_file\u001b[39m=\u001b[39;49mCONFIG_FILE)\n",
      "File \u001b[0;32m/media/matthieu/LaCie/2-mpa/run_experiment.py:124\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(config_file, csv_path, data_dir, writer)\u001b[0m\n\u001b[1;32m    115\u001b[0m         torchinfo\u001b[39m.\u001b[39msummary(model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m    116\u001b[0m         input_size\u001b[39m=\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m7\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m), \u001b[39m# make sure this is \"input_size\", not \"input_shape\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         \u001b[39m# col_names=[\"input_size\"], # uncomment for smaller output\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m         row_settings\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mvar_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    121\u001b[0m )           \n\u001b[1;32m    122\u001b[0m         \u001b[39m# TRAINING\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         results[fold] \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 124\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: train(\n\u001b[1;32m    125\u001b[0m                 model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    126\u001b[0m                 train_dataloader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m    127\u001b[0m                 val_dataloader\u001b[39m=\u001b[39;49mval_loader,\n\u001b[1;32m    128\u001b[0m                 optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    129\u001b[0m                 scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m    130\u001b[0m                 loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    131\u001b[0m                 epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mn_epochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    132\u001b[0m                 batch_size\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    133\u001b[0m                 in_channels\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39min_channels\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    134\u001b[0m                 writer\u001b[39m=\u001b[39;49mwriter,\n\u001b[1;32m    135\u001b[0m                 device\u001b[39m=\u001b[39;49mDEVICE,\n\u001b[1;32m    136\u001b[0m                 r2\u001b[39m=\u001b[39;49mr2\n\u001b[1;32m    137\u001b[0m             ),\n\u001b[1;32m    138\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: test(\n\u001b[1;32m    139\u001b[0m                 model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    140\u001b[0m                 dataloader\u001b[39m=\u001b[39mtest_loader,\n\u001b[1;32m    141\u001b[0m                 device\u001b[39m=\u001b[39mDEVICE\n\u001b[1;32m    142\u001b[0m             )\n\u001b[1;32m    143\u001b[0m         }\n\u001b[1;32m    144\u001b[0m         \u001b[39m# Clear GPU cache memory\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         utils\u001b[39m.\u001b[39mfree_gpu_cache()\n",
      "File \u001b[0;32m/media/matthieu/LaCie/2-mpa/train.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, scheduler, loss_fn, epochs, batch_size, in_channels, device, writer, r2)\u001b[0m\n\u001b[1;32m    176\u001b[0m     writer\u001b[39m.\u001b[39madd_scalars(main_tag\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mR2\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    177\u001b[0m                        tag_scalar_dict\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtrain_r2\u001b[39m\u001b[39m\"\u001b[39m: train_r2,\n\u001b[1;32m    178\u001b[0m                                         \u001b[39m\"\u001b[39m\u001b[39mtest_r2\u001b[39m\u001b[39m\"\u001b[39m: test_r2}, \n\u001b[1;32m    179\u001b[0m                        global_step\u001b[39m=\u001b[39mepoch)\n\u001b[1;32m    181\u001b[0m     \u001b[39m# Track the PyTorch model architecture\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     writer\u001b[39m.\u001b[39;49madd_graph(model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m    183\u001b[0m                      \u001b[39m# Pass in an example input\u001b[39;49;00m\n\u001b[1;32m    184\u001b[0m                      input_to_model\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mrandn(batch_size, in_channels, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m    186\u001b[0m \u001b[39m# Close the writer\u001b[39;00m\n\u001b[1;32m    187\u001b[0m writer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:841\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    837\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mtensorboard.logging.add_graph\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    839\u001b[0m     \u001b[39m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_graph(\n\u001b[0;32m--> 841\u001b[0m         graph(model, input_to_model, verbose, use_strict_trace)\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[39m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m caffe2_pb2\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/tensorboard/_pytorch_graph.py:343\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m    342\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError occurs, No graph saved\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 343\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    346\u001b[0m     \u001b[39mprint\u001b[39m(graph)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/tensorboard/_pytorch_graph.py:337\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    336\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m         trace \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(model, args, strict\u001b[39m=\u001b[39;49muse_strict_trace)\n\u001b[1;32m    338\u001b[0m         graph \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mgraph\n\u001b[1;32m    339\u001b[0m         torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/jit/_trace.py:759\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 759\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    760\u001b[0m         func,\n\u001b[1;32m    761\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    762\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    763\u001b[0m         check_trace,\n\u001b[1;32m    764\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    765\u001b[0m         check_tolerance,\n\u001b[1;32m    766\u001b[0m         strict,\n\u001b[1;32m    767\u001b[0m         _force_outplace,\n\u001b[1;32m    768\u001b[0m         _module_class,\n\u001b[1;32m    769\u001b[0m     )\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    772\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    773\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    774\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    777\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    778\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m         _module_class,\n\u001b[1;32m    786\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/jit/_trace.py:976\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    972\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    974\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 976\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m    977\u001b[0m     method_name,\n\u001b[1;32m    978\u001b[0m     func,\n\u001b[1;32m    979\u001b[0m     example_inputs,\n\u001b[1;32m    980\u001b[0m     var_lookup_fn,\n\u001b[1;32m    981\u001b[0m     strict,\n\u001b[1;32m    982\u001b[0m     _force_outplace,\n\u001b[1;32m    983\u001b[0m     argument_names,\n\u001b[1;32m    984\u001b[0m )\n\u001b[1;32m    985\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    987\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1183\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/timm/models/resnet.py:675\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 675\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    676\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_pool(x)\n\u001b[1;32m    677\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/timm/models/resnet.py:671\u001b[0m, in \u001b[0;36mResNet.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    669\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    670\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 671\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[1;32m    672\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1183\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1183\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/timm/models/resnet.py:331\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maa \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maa(x)\n\u001b[0;32m--> 331\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m    332\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_block \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1183\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 22.02 GiB already allocated; 20.62 MiB free; 22.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "results = run_experiment(config_file=CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa0e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e292ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
