{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee11b763",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065c3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117e1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.from_config import build_from_config\n",
    "from models.double_branch import DoubleBranchCNN\n",
    "from data_handlers.csv_dataset import CustomDatasetFromDataFrame\n",
    "from utils import utils\n",
    "from utils import transfer_learning as tl\n",
    "from train import train, dual_train\n",
    "from test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH=os.path.join('data','dataset.csv')\n",
    "DATA_DIR=os.path.join('data','landsat_7','')\n",
    "FOLD_PATH=os.path.join('data','dhs_incountry_folds.pkl')\n",
    "CONFIG_FILE_MS = os.path.join('configs','resnet18_ms_e2e_l7_yeh.json')\n",
    "CONFIG_FILE_MSNL = os.path.join('configs','resnet18_msnl_e2e_l7_yeh.json')\n",
    "TILE_MIN = [-0.0994, -0.0574, -0.0318, -0.0209, -0.0102, -0.0152, 0.0, -0.07087274]\n",
    "TILE_MAX = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 316.7, 3104.1401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5da8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>cluster</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>households</th>\n",
       "      <th>wealthpooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.350257</td>\n",
       "      <td>13.534922</td>\n",
       "      <td>36</td>\n",
       "      <td>2.312757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>-12.360865</td>\n",
       "      <td>13.551494</td>\n",
       "      <td>32</td>\n",
       "      <td>2.010293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>-12.613421</td>\n",
       "      <td>13.413085</td>\n",
       "      <td>36</td>\n",
       "      <td>0.877744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>-12.581454</td>\n",
       "      <td>13.397711</td>\n",
       "      <td>35</td>\n",
       "      <td>1.066994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>-12.578135</td>\n",
       "      <td>13.418748</td>\n",
       "      <td>37</td>\n",
       "      <td>1.750153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  year  cluster        lat        lon  households  wealthpooled\n",
       "0  angola  2011        1 -12.350257  13.534922          36      2.312757\n",
       "1  angola  2011        2 -12.360865  13.551494          32      2.010293\n",
       "2  angola  2011        3 -12.613421  13.413085          36      0.877744\n",
       "3  angola  2011        4 -12.581454  13.397711          35      1.066994\n",
       "4  angola  2011        5 -12.578135  13.418748          37      1.750153"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open( CONFIG_FILE_MS ) as f:\n",
    "    config_ms = json.load(f)\n",
    "with open( CONFIG_FILE_MSNL ) as f:\n",
    "    config_msnl = json.load(f)\n",
    "csv = pd.read_csv(CSV_PATH)\n",
    "# csv.drop(\"bounding_box\", axis=1, inplace=True)\n",
    "# csv = csv.loc[:, ~csv.columns.str.contains('^Unnamed')]\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfeb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRANSFORM  = torch.nn.Sequential(\n",
    "        torchvision.transforms.CenterCrop(size=224),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28af04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE THE MEAN AND STD OF NORMED IMAGES OVER THE COMPLETE DATASET\n",
    "# EXECUTE ONCE -> to script\n",
    "dummy_dataset = CustomDatasetFromDataFrame(csv,\n",
    "                                           DATA_DIR,\n",
    "                                           transform=TEST_TRANSFORM,\n",
    "                                           tile_max=TILE_MAX,\n",
    "                                           tile_min=TILE_MIN)\n",
    "dummy_loader = torch.utils.data.DataLoader(\n",
    "        dummy_dataset, \n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "def compute_mean_and_std(dataloader, batch_size):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        if data is not None:\n",
    "            weight = data.size()[0] / batch_size\n",
    "            # Mean over batch, height and width, but not over the channels\n",
    "            channels_sum += weight*torch.mean(data, dim=[0,2,3])\n",
    "            channels_squared_sum += weight*torch.mean(data**2, dim=[0,2,3])\n",
    "            num_batches += weight\n",
    "    mean = channels_sum / num_batches\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "    return mean, std\n",
    "\n",
    "means, stds = compute_mean_and_std(dummy_loader, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a375ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means, stds\n",
    "means = torch.tensor([0.6952, 0.6890, 0.6851, 0.6834, 0.6818, 0.6826, 0.0043])\n",
    "stds = torch.tensor([9.5266, 9.7209, 9.8435, 9.8968, 9.9495, 9.9249, 0.0632])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3db94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TRANSFORM = torch.nn.Sequential(\n",
    "        torchvision.transforms.CenterCrop(size=224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=means,\n",
    "            std=stds\n",
    "        )\n",
    "    )\n",
    "TEST_TRANSFORM  = torch.nn.Sequential(\n",
    "        torchvision.transforms.CenterCrop(size=224),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=means,\n",
    "            std=stds\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20462eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 10:45:01.913268: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 10:45:01.989558: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 32463 32464 32465]\n",
      "Training on fold (All)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:26<00:00,  3.11s/it]  \n",
      "100%|██████████| 305/305 [15:50<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.5879 | train_r2: 0.1175 | test_loss: 0.5792 | test_r2: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:06<00:00,  3.09s/it]  \n",
      "100%|██████████| 305/305 [15:32<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.5330 | train_r2: 0.2027 | test_loss: 1.2296 | test_r2: -0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:39<00:00,  3.13s/it]  \n",
      "100%|██████████| 305/305 [15:43<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.5364 | train_r2: 0.1979 | test_loss: 0.7378 | test_r2: -0.1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:37<00:00,  3.06s/it]  \n",
      "100%|██████████| 305/305 [15:32<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.4603 | train_r2: 0.3082 | test_loss: 2.8147 | test_r2: -3.4065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:43<00:00,  3.07s/it]  \n",
      "100%|██████████| 305/305 [15:33<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.4782 | train_r2: 0.2824 | test_loss: 1.9788 | test_r2: -2.1039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:06<00:00,  3.09s/it]  \n",
      "100%|██████████| 305/305 [15:29<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.4394 | train_r2: 0.3412 | test_loss: 2232.1237 | test_r2: -3483.1997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:45<00:00,  3.07s/it]  \n",
      "100%|██████████| 305/305 [15:34<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss: 0.4735 | train_r2: 0.2888 | test_loss: 1.4861 | test_r2: -1.3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:55<00:00,  3.08s/it]  \n",
      "100%|██████████| 305/305 [15:38<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss: 0.4746 | train_r2: 0.2871 | test_loss: 1.3395 | test_r2: -1.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:37<00:00,  3.06s/it]  \n",
      "100%|██████████| 305/305 [15:32<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss: 0.3864 | train_r2: 0.4176 | test_loss: 1.4174 | test_r2: -1.2131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:04<00:00,  3.09s/it]  \n",
      "100%|██████████| 305/305 [15:33<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss: 0.4013 | train_r2: 0.3952 | test_loss: 5.2470 | test_r2: -7.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:15<00:00,  3.10s/it]  \n",
      "100%|██████████| 305/305 [15:38<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss: 0.3657 | train_r2: 0.4495 | test_loss: 1.5089 | test_r2: -1.3245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:44<00:00,  3.07s/it]  \n",
      "100%|██████████| 305/305 [15:34<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | train_loss: 0.3609 | train_r2: 0.4566 | test_loss: 0.7637 | test_r2: -0.1813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:54<00:00,  3.08s/it]  \n",
      "100%|██████████| 305/305 [15:34<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | train_loss: 0.3459 | train_r2: 0.4789 | test_loss: 0.5348 | test_r2: 0.1754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:02<00:00,  3.09s/it]  \n",
      "100%|██████████| 305/305 [15:32<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | train_loss: 0.3428 | train_r2: 0.4832 | test_loss: 1.5077 | test_r2: -1.3656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:51<00:00,  3.08s/it]  \n",
      "100%|██████████| 305/305 [15:37<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | train_loss: 0.3415 | train_r2: 0.4854 | test_loss: 5.7095 | test_r2: -7.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:11<00:00,  3.10s/it]  \n",
      "100%|██████████| 305/305 [15:37<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | train_loss: 0.3405 | train_r2: 0.4878 | test_loss: 6.3562 | test_r2: -8.8737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:05<00:00,  3.09s/it]  \n",
      "100%|██████████| 305/305 [15:35<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | train_loss: 0.3404 | train_r2: 0.4864 | test_loss: 0.5480 | test_r2: 0.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:10<00:00,  3.10s/it]  \n",
      "100%|██████████| 305/305 [15:38<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | train_loss: 0.3396 | train_r2: 0.4883 | test_loss: 0.5592 | test_r2: 0.1345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:04<00:00,  3.09s/it]  \n",
      "100%|██████████| 305/305 [15:34<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | train_loss: 0.3368 | train_r2: 0.4926 | test_loss: 1.2724 | test_r2: -0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:45<00:00,  3.07s/it]  \n",
      "100%|██████████| 305/305 [15:35<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | train_loss: 0.3363 | train_r2: 0.4930 | test_loss: 2.3322 | test_r2: -2.6399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [46:53<00:00,  3.08s/it]  \n",
      "100%|██████████| 305/305 [15:36<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | train_loss: 0.3375 | train_r2: 0.4912 | test_loss: 0.3386 | test_r2: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:34<00:00,  3.12s/it]  \n",
      "100%|██████████| 305/305 [15:46<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | train_loss: 0.3342 | train_r2: 0.4974 | test_loss: 0.9170 | test_r2: -0.4323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [47:41<00:00,  3.13s/it]  \n",
      "100%|██████████| 305/305 [16:07<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | train_loss: 0.3360 | train_r2: 0.4937 | test_loss: 1.8765 | test_r2: -1.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 759/914 [39:39<01:55,  1.34it/s]  "
     ]
    }
   ],
   "source": [
    "# Spatially Aware Cross-Validation\n",
    "with open(FOLD_PATH, 'rb') as f:\n",
    "    folds = pickle.load(f)\n",
    "results = dict()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# for fold in folds:\n",
    "writer = SummaryWriter()\n",
    "r2 = torchmetrics.R2Score().to(device=device)\n",
    "# Index split\n",
    "\n",
    "train_split = np.concatenate((folds['A']['train'],folds['B']['train'],folds['C']['train']))\n",
    "val_split = folds['D']['train']\n",
    "print(train_split)\n",
    "# CSV split\n",
    "train_df = csv.iloc[train_split]\n",
    "val_df = csv.iloc[val_split]\n",
    "# Datasets\n",
    "train_dataset = CustomDatasetFromDataFrame(train_df, DATA_DIR,transform=TRAIN_TRANSFORM,tile_max=TILE_MAX,\n",
    "                                        tile_min=TILE_MIN )\n",
    "val_dataset = CustomDatasetFromDataFrame(val_df, DATA_DIR, transform=TEST_TRANSFORM,tile_max=TILE_MAX,\n",
    "                                        tile_min=TILE_MIN )\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config_ms['batch_size'], \n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config_ms['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "base_model = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "# base_model = torchgeo.models.resnet18(weights=torchgeo.models.ResNet18_Weights.SENTINEL2_ALL_MOCO)\n",
    "ms_branch = build_from_config( base_model=base_model, config_file=CONFIG_FILE_MS )\n",
    "# nl_branch = tl.update_single_layer(torchvision.models.resnet18())\n",
    "# model = DoubleBranchCNN(b1=ms_branch, b2=nl_branch, output_features=1)\n",
    "model = ms_branch.to(device=device)\n",
    "# CONFIGURE LOSS, OPTIM\n",
    "loss_fn = utils.configure_loss( config_ms )\n",
    "optimizer = utils.configure_optimizer( config_ms, model )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer)\n",
    "# print(f\"Training on fold {fold}\")\n",
    "print(f\"Training on fold (All)\")\n",
    "results = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=config_ms['n_epochs'],\n",
    "    batch_size=config_ms['batch_size'],\n",
    "    in_channels=config_ms['in_channels'],\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    r2=r2\n",
    ")\n",
    "torch.save(model.state_dict(), config_ms['checkpoint_path']+'_fold_'+'all'+\".pth\")\n",
    "# final_results = utils.compute_average_crossval_results(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e37d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dabce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatially Aware Cross-Validation\n",
    "with open(FOLD_PATH, 'rb') as f:\n",
    "    folds = pickle.load(f)\n",
    "results = dict()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "for fold in folds:\n",
    "    writer = SummaryWriter()\n",
    "    r2 = torchmetrics.R2Score().to(device=device)\n",
    "    # Index split\n",
    "    train_split = folds[fold]['train']\n",
    "    val_split = folds[fold]['val']\n",
    "    test_split = folds[fold]['test']\n",
    "    # CSV split\n",
    "    train_df = csv.iloc[train_split]\n",
    "    val_df = csv.iloc[train_split]\n",
    "    test_df = csv.iloc[test_split]\n",
    "    # Datasets\n",
    "    train_dataset = CustomDatasetFromDataFrame(train_df, DATA_DIR,transform=TRAIN_TRANSFORM )\n",
    "    val_dataset = CustomDatasetFromDataFrame(val_df, DATA_DIR, transform=TEST_TRANSFORM )\n",
    "    test_dataset  = CustomDatasetFromDataFrame(test_df, DATA_DIR, transform=TEST_TRANSFORM )\n",
    "    # DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config_msnl['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config_msnl['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config_msnl['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    base_model = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "    # base_model = torchgeo.models.resnet18(weights=torchgeo.models.ResNet18_Weights.SENTINEL2_ALL_MOCO)\n",
    "    ms_branch = build_from_config( base_model=base_model, config_file=CONFIG_FILE_MSNL )\n",
    "    nl_branch = tl.update_single_layer(torchvision.models.resnet18())\n",
    "    model = DoubleBranchCNN(b1=ms_branch, b2=nl_branch, output_features=1)\n",
    "    model = model.to(device=device)\n",
    "    # CONFIGURE LOSS, OPTIM\n",
    "    loss_fn = utils.configure_loss( config_msnl )\n",
    "    optimizer = utils.configure_optimizer( config_msnl, model )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer)\n",
    "    print(f\"Training on fold {fold}\")\n",
    "    results[fold] = dual_train(\n",
    "        model=model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=config_msnl['n_epochs'],\n",
    "        batch_size=config_msnl['batch_size'],\n",
    "        in_channels=config_msnl['in_channels'],\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        r2=r2\n",
    "    )\n",
    "    torch.save(model.state_dict(), config_msnl['checkpoint_path']+'_fold_'+str(fold)+\".pth\")\n",
    "final_results_nl = utils.compute_average_crossval_results(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f428df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b5e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac9b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb8ae5a",
   "metadata": {},
   "source": [
    "3. Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db74dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_r2, Y_true, Y_pred = test(model=model, dataloader=val_loader, device=device)\n",
    "# # Y_true = [ utils.denormalize_asset(asset) for asset in Y_true]\n",
    "# # Y_pred = [ utils.denormalize_asset(asset) for asset in Y_pred]\n",
    "# results = pd.DataFrame({\n",
    "#     'true index':np.array(Y_true),\n",
    "#     'predicted index':np.array(Y_pred)\n",
    "# })\n",
    "# from scipy.stats import pearsonr\n",
    "# import seaborn as sns\n",
    "# sns.set_palette(\"rocket\")\n",
    "# sns.regplot(x='true index', y='predicted index', data=results).set(title='R2 = '+str(test_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68768d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ed5c42258d104453582e2fee2faf5d01150c2a161fd6cc7123c0fdfe444c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
