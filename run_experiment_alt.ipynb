{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee11b763",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065c3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117e1d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.from_config import build_from_config\n",
    "from models.double_branch import DoubleBranchCNN\n",
    "from data_handlers.csv_dataset import CustomDatasetFromDataFrame\n",
    "from utils import utils\n",
    "from utils import transfer_learning as tl\n",
    "from train import train, dual_train\n",
    "from test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH=os.path.join('data','dataset.csv')\n",
    "DATA_DIR=os.path.join('data','landsat_7','')\n",
    "FOLD_PATH=os.path.join('data','dhs_incountry_folds.pkl')\n",
    "CONFIG_FILE_MS = os.path.join('configs','resnet18_ms_e2e_l7_yeh.json')\n",
    "CONFIG_FILE_MSNL = os.path.join('configs','resnet18_msnl_e2e_l7_yeh.json')\n",
    "TILE_MIN = [-0.0994, -0.0574, -0.0318, -0.0209, -0.0102, -0.0152, 0.0, -0.07087274]\n",
    "TILE_MAX = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 316.7, 3104.1401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5da8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>cluster</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>households</th>\n",
       "      <th>wealthpooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.350257</td>\n",
       "      <td>13.534922</td>\n",
       "      <td>36</td>\n",
       "      <td>2.312757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>-12.360865</td>\n",
       "      <td>13.551494</td>\n",
       "      <td>32</td>\n",
       "      <td>2.010293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>-12.613421</td>\n",
       "      <td>13.413085</td>\n",
       "      <td>36</td>\n",
       "      <td>0.877744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>-12.581454</td>\n",
       "      <td>13.397711</td>\n",
       "      <td>35</td>\n",
       "      <td>1.066994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angola</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>-12.578135</td>\n",
       "      <td>13.418748</td>\n",
       "      <td>37</td>\n",
       "      <td>1.750153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  year  cluster        lat        lon  households  wealthpooled\n",
       "0  angola  2011        1 -12.350257  13.534922          36      2.312757\n",
       "1  angola  2011        2 -12.360865  13.551494          32      2.010293\n",
       "2  angola  2011        3 -12.613421  13.413085          36      0.877744\n",
       "3  angola  2011        4 -12.581454  13.397711          35      1.066994\n",
       "4  angola  2011        5 -12.578135  13.418748          37      1.750153"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open( CONFIG_FILE_MS ) as f:\n",
    "    config_ms = json.load(f)\n",
    "with open( CONFIG_FILE_MSNL ) as f:\n",
    "    config_msnl = json.load(f)\n",
    "csv = pd.read_csv(CSV_PATH)\n",
    "# csv.drop(\"bounding_box\", axis=1, inplace=True)\n",
    "# csv = csv.loc[:, ~csv.columns.str.contains('^Unnamed')]\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfeb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TRANSFORM = torch.nn.Sequential(\n",
    "        torchvision.transforms.CenterCrop(size=224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=torch.tensor([0.0761, 0.0722, 0.0687, 0.1369, 0.1298, 0.0933, 0.9453, 0.0017]),\n",
    "            std=torch.tensor([0.0119, 0.0171, 0.0277, 0.0304, 0.0476, 0.0443, 0.0193, 0.0073])\n",
    "        )\n",
    "    )\n",
    "TEST_TRANSFORM  = torch.nn.Sequential(\n",
    "        torchvision.transforms.CenterCrop(size=224),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28af04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE THE MEAN AND STD OF NORMED IMAGES OVER THE COMPLETE DATASET\n",
    "# EXECUTE ONCE -> to script\n",
    "dummy_dataset = CustomDatasetFromDataFrame(csv,\n",
    "                                           DATA_DIR,\n",
    "                                           transform=TEST_TRANSFORM,\n",
    "                                           tile_max=TILE_MAX,\n",
    "                                           tile_min=TILE_MIN)\n",
    "dummy_loader = torch.utils.data.DataLoader(\n",
    "        dummy_dataset, \n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "def compute_mean_and_std(dataloader, batch_size):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        if data is not None:\n",
    "            weight = data.size()[0] / batch_size\n",
    "            # Mean over batch, height and width, but not over the channels\n",
    "            channels_sum += weight*torch.mean(data, dim=[0,2,3])\n",
    "            channels_squared_sum += weight*torch.mean(data**2, dim=[0,2,3])\n",
    "            num_batches += weight\n",
    "    mean = channels_sum / num_batches\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "    return mean, std\n",
    "\n",
    "means, stds = compute_mean_and_std(dummy_loader, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375ed2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0761, 0.0722, 0.0687, 0.1369, 0.1298, 0.0933, 0.9453, 0.0017]),\n",
       " tensor([0.0119, 0.0171, 0.0277, 0.0304, 0.0476, 0.0443, 0.0193, 0.0073]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20462eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 16:09:21.481710: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 16:09:21.563133: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [38:34<00:00,  7.77s/it] \n",
      "100%|██████████| 298/298 [37:47<00:00,  7.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.4193 | train_r2: 0.3875 | test_loss: 3.0923 | test_r2: -3.6118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:48<00:00,  7.61s/it] \n",
      "100%|██████████| 298/298 [37:45<00:00,  7.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.2905 | train_r2: 0.5725 | test_loss: 0.7304 | test_r2: -0.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:47<00:00,  7.61s/it] \n",
      "100%|██████████| 298/298 [37:41<00:00,  7.59s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.3006 | train_r2: 0.5566 | test_loss: 2.4707 | test_r2: -2.6645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:52<00:00,  7.63s/it] \n",
      "100%|██████████| 298/298 [37:45<00:00,  7.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.3089 | train_r2: 0.5447 | test_loss: 1.7030 | test_r2: -1.5376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:53<00:00,  7.63s/it] \n",
      "100%|██████████| 298/298 [37:46<00:00,  7.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.2932 | train_r2: 0.5688 | test_loss: 3.0685 | test_r2: -3.6024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:51<00:00,  7.62s/it]  \n",
      "100%|██████████| 298/298 [37:46<00:00,  7.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.2930 | train_r2: 0.5671 | test_loss: 1.1945 | test_r2: -0.7714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:50<00:00,  7.62s/it] \n",
      "100%|██████████| 298/298 [37:45<00:00,  7.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss: 0.2877 | train_r2: 0.5771 | test_loss: 1.2923 | test_r2: -0.9099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:49<00:00,  7.62s/it] \n",
      "100%|██████████| 298/298 [37:44<00:00,  7.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss: 0.2754 | train_r2: 0.5945 | test_loss: 2.0312 | test_r2: -2.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:48<00:00,  7.61s/it]  \n",
      "100%|██████████| 298/298 [37:45<00:00,  7.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss: 0.2738 | train_r2: 0.5973 | test_loss: 2.1926 | test_r2: -2.2529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:48<00:00,  7.61s/it] \n",
      "100%|██████████| 298/298 [37:43<00:00,  7.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss: 0.2752 | train_r2: 0.5933 | test_loss: 2.0814 | test_r2: -2.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:49<00:00,  7.62s/it] \n",
      "100%|██████████| 298/298 [37:43<00:00,  7.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss: 0.2808 | train_r2: 0.5867 | test_loss: 2.2133 | test_r2: -2.2960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:53<00:00,  7.63s/it] \n",
      "100%|██████████| 298/298 [37:48<00:00,  7.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | train_loss: 0.2730 | train_r2: 0.5995 | test_loss: 2.0053 | test_r2: -1.9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [37:50<00:00,  7.62s/it] \n",
      "100%|██████████| 298/298 [37:45<00:00,  7.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | train_loss: 0.2706 | train_r2: 0.6021 | test_loss: 2.8309 | test_r2: -3.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 80/298 [10:33<28:45,  7.92s/it]  \n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "Caught RasterioIOError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 308, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 219, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: 'data/landsat_7/cote_d_ivoire_2012_160_09948.tif' not recognized as a supported file format.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/media/matthieu/LaCie/2-mpa/data_handlers/csv_dataset.py\", line 39, in __getitem__\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: 'data/landsat_7/cote_d_ivoire_2012_160_09948.tif' not recognized as a supported file format.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m     scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining on fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     results[fold] \u001b[39m=\u001b[39m train(\n\u001b[1;32m     58\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     59\u001b[0m         train_dataloader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     60\u001b[0m         val_dataloader\u001b[39m=\u001b[39;49mval_loader,\n\u001b[1;32m     61\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     62\u001b[0m         scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     63\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     64\u001b[0m         epochs\u001b[39m=\u001b[39;49mconfig_ms[\u001b[39m'\u001b[39;49m\u001b[39mn_epochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     65\u001b[0m         batch_size\u001b[39m=\u001b[39;49mconfig_ms[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     66\u001b[0m         in_channels\u001b[39m=\u001b[39;49mconfig_ms[\u001b[39m'\u001b[39;49m\u001b[39min_channels\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     67\u001b[0m         writer\u001b[39m=\u001b[39;49mwriter,\n\u001b[1;32m     68\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     69\u001b[0m         r2\u001b[39m=\u001b[39;49mr2\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), config_ms[\u001b[39m'\u001b[39m\u001b[39mcheckpoint_path\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_fold_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(fold)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m final_results \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcompute_average_crossval_results(results\u001b[39m=\u001b[39mresults)\n",
      "File \u001b[0;32m/media/matthieu/LaCie/2-mpa/train.py:138\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, scheduler, loss_fn, epochs, batch_size, in_channels, device, writer, r2)\u001b[0m\n",
      "File \u001b[0;32m/media/matthieu/LaCie/2-mpa/train.py:23\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device, r2)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: Caught RasterioIOError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 308, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 219, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: 'data/landsat_7/cote_d_ivoire_2012_160_09948.tif' not recognized as a supported file format.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/media/matthieu/LaCie/2-mpa/data_handlers/csv_dataset.py\", line 39, in __getitem__\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/matthieu/anaconda3/envs/mpa_env/lib/python3.10/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: 'data/landsat_7/cote_d_ivoire_2012_160_09948.tif' not recognized as a supported file format.\n"
     ]
    }
   ],
   "source": [
    "# Spatially Aware Cross-Validation\n",
    "with open(FOLD_PATH, 'rb') as f:\n",
    "    folds = pickle.load(f)\n",
    "results = dict()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "for fold in folds:\n",
    "    writer = SummaryWriter()\n",
    "    r2 = torchmetrics.R2Score().to(device=device)\n",
    "    # Index split\n",
    "    train_split = folds[fold]['train']\n",
    "    val_split = folds[fold]['val']\n",
    "    test_split = folds[fold]['test']\n",
    "    # CSV split\n",
    "    train_df = csv.iloc[train_split]\n",
    "    val_df = csv.iloc[train_split]\n",
    "    test_df = csv.iloc[test_split]\n",
    "    # Datasets\n",
    "    train_dataset = CustomDatasetFromDataFrame(train_df, DATA_DIR,transform=TRAIN_TRANSFORM,tile_max=TILE_MAX,\n",
    "                                           tile_min=TILE_MIN )\n",
    "    val_dataset = CustomDatasetFromDataFrame(val_df, DATA_DIR, transform=TEST_TRANSFORM,tile_max=TILE_MAX,\n",
    "                                           tile_min=TILE_MIN )\n",
    "    test_dataset  = CustomDatasetFromDataFrame(test_df, DATA_DIR, transform=TEST_TRANSFORM,tile_max=TILE_MAX,\n",
    "                                           tile_min=TILE_MIN)\n",
    "    # DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config_ms['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config_ms['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config_ms['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    base_model = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "    # base_model = torchgeo.models.resnet18(weights=torchgeo.models.ResNet18_Weights.SENTINEL2_ALL_MOCO)\n",
    "    ms_branch = build_from_config( base_model=base_model, config_file=CONFIG_FILE_MS )\n",
    "    # nl_branch = tl.update_single_layer(torchvision.models.resnet18())\n",
    "    # model = DoubleBranchCNN(b1=ms_branch, b2=nl_branch, output_features=1)\n",
    "    model = ms_branch.to(device=device)\n",
    "    # CONFIGURE LOSS, OPTIM\n",
    "    loss_fn = utils.configure_loss( config_ms )\n",
    "    optimizer = utils.configure_optimizer( config_ms, model )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer)\n",
    "    print(f\"Training on fold {fold}\")\n",
    "    results[fold] = train(\n",
    "        model=model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=config_ms['n_epochs'],\n",
    "        batch_size=config_ms['batch_size'],\n",
    "        in_channels=config_ms['in_channels'],\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        r2=r2\n",
    "    )\n",
    "    torch.save(model.state_dict(), config_ms['checkpoint_path']+'_fold_'+str(fold)+\".pth\")\n",
    "final_results = utils.compute_average_crossval_results(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e37d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dabce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatially Aware Cross-Validation\n",
    "with open(FOLD_PATH, 'rb') as f:\n",
    "    folds = pickle.load(f)\n",
    "results = dict()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "for fold in folds:\n",
    "    writer = SummaryWriter()\n",
    "    r2 = torchmetrics.R2Score().to(device=device)\n",
    "    # Index split\n",
    "    train_split = folds[fold]['train']\n",
    "    val_split = folds[fold]['val']\n",
    "    test_split = folds[fold]['test']\n",
    "    # CSV split\n",
    "    train_df = csv.iloc[train_split]\n",
    "    val_df = csv.iloc[train_split]\n",
    "    test_df = csv.iloc[test_split]\n",
    "    # Datasets\n",
    "    train_dataset = CustomDatasetFromDataFrame(train_df, DATA_DIR,transform=TRAIN_TRANSFORM )\n",
    "    val_dataset = CustomDatasetFromDataFrame(val_df, DATA_DIR, transform=TEST_TRANSFORM )\n",
    "    test_dataset  = CustomDatasetFromDataFrame(test_df, DATA_DIR, transform=TEST_TRANSFORM )\n",
    "    # DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config_msnl['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config_msnl['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config_msnl['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    base_model = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "    # base_model = torchgeo.models.resnet18(weights=torchgeo.models.ResNet18_Weights.SENTINEL2_ALL_MOCO)\n",
    "    ms_branch = build_from_config( base_model=base_model, config_file=CONFIG_FILE_MSNL )\n",
    "    nl_branch = tl.update_single_layer(torchvision.models.resnet18())\n",
    "    model = DoubleBranchCNN(b1=ms_branch, b2=nl_branch, output_features=1)\n",
    "    model = model.to(device=device)\n",
    "    # CONFIGURE LOSS, OPTIM\n",
    "    loss_fn = utils.configure_loss( config_msnl )\n",
    "    optimizer = utils.configure_optimizer( config_msnl, model )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer)\n",
    "    print(f\"Training on fold {fold}\")\n",
    "    results[fold] = dual_train(\n",
    "        model=model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=config_msnl['n_epochs'],\n",
    "        batch_size=config_msnl['batch_size'],\n",
    "        in_channels=config_msnl['in_channels'],\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        r2=r2\n",
    "    )\n",
    "    torch.save(model.state_dict(), config_msnl['checkpoint_path']+'_fold_'+str(fold)+\".pth\")\n",
    "final_results_nl = utils.compute_average_crossval_results(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f428df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b5e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac9b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb8ae5a",
   "metadata": {},
   "source": [
    "3. Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db74dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_r2, Y_true, Y_pred = test(model=model, dataloader=val_loader, device=device)\n",
    "# # Y_true = [ utils.denormalize_asset(asset) for asset in Y_true]\n",
    "# # Y_pred = [ utils.denormalize_asset(asset) for asset in Y_pred]\n",
    "# results = pd.DataFrame({\n",
    "#     'true index':np.array(Y_true),\n",
    "#     'predicted index':np.array(Y_pred)\n",
    "# })\n",
    "# from scipy.stats import pearsonr\n",
    "# import seaborn as sns\n",
    "# sns.set_palette(\"rocket\")\n",
    "# sns.regplot(x='true index', y='predicted index', data=results).set(title='R2 = '+str(test_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68768d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ed5c42258d104453582e2fee2faf5d01150c2a161fd6cc7123c0fdfe444c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
